{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwcjKK7cI8gH"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Contrastive Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our experiments are running in the previous lab, let's take a moment to learn about an important multimodal technique: contrastive pre-training. This technique is famously used to make Contrastive Language-Image Pre-training, also known as, CLIP. Contrastive Pre-training is not limited to comparing language and image data. So, in this lab, we will be exploring how to make a contrastive pre-training model for any two correlated datatypes.\n",
    "\n",
    "#### Learning Objectives:\n",
    "The goals of this notebook are to:\n",
    "* Apply computer vision techniques such as the Sobel filter to create a dataset\n",
    "* Calculate the cosine similarity between two vectors\n",
    "* Create a contrastive pre-training model from two embedding models\n",
    "* Use the contrastive pre-training model to create a vector database\n",
    "\n",
    "To begin, let's load the libraries needed for this course."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xxMuSaaKl-JA",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Visualization tools\n",
    "import graphviz\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cmap = \"gray\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyA2xmmjI72c"
   },
   "source": [
    "## 2.1 The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will be using the [FashionMNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) dataset. We can use contrastive pre-training models to create vector databases, which are commonly used in retrieval augmented generation (RAG) pipelines. Let's create a database where users can look up an article of clothing based on a sketched outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "771gMWaaQtAg",
    "outputId": "e69d15f3-7bb1-4b33-ae59-90ca86cf2f67"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.FashionMNIST(\n",
    "    \"./data/\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "rGYPRRNiRT4d",
    "outputId": "8f93456c-2a2e-4557-9182-22a6b1e5cd09"
   },
   "outputs": [],
   "source": [
    "def show_images(dataset, num_samples=10):\n",
    "    plt.figure(figsize=(16, 1))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            return\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(torch.squeeze(img[0]), cmap=cmap)\n",
    "\n",
    "show_images(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvBbUkEIKn_V"
   },
   "source": [
    "We can generate the image outlines by using a [Sobel filter](https://medium.com/@deepika.vadlamudi/implementing-a-sobel-filter-with-cuda-in-python-2b9b18485e31). `Gx` is a convolution kernel that will identify horizontal edges, and `Gy` will identify vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1FnUsOa5L2v",
    "outputId": "30bb30b4-1bc3-4827-e9a4-ed2753d5e42f"
   },
   "outputs": [],
   "source": [
    "Gx = torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], dtype=torch.float)\n",
    "Gy = torch.tensor([[[[1, 2, 1], [0, 0, 0], [-1, -2, -1]]]], dtype=torch.float)\n",
    "print(\"Gx:\")\n",
    "print(Gx)\n",
    "print(\"Gy:\")\n",
    "print(Gy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_data[0][0][0]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a small amount of noise in the fashionMNIST images. We can set any pixel value below a threshold to 0, and everything above the threshold to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.25\n",
    "new_img = img.clone()\n",
    "new_img[new_img > threshold] = 1\n",
    "new_img[new_img <= threshold] = 0\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can apply to Sobel filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_x = F.conv2d(new_img[None, :,:], Gx, stride=1, padding=1)\n",
    "edges_y = F.conv2d(new_img[None, :,:], Gy, stride=1, padding=1)\n",
    "edges = edges_x + edges_y\n",
    "edges = edges[0] # Remove batch\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og_d2w9VUQfT"
   },
   "source": [
    "Because the filters are asymmetrical, some parts of the outline will be positive, and some parts will be negative. We will set all nonzero values to 1 in order to have a consistent outline. The result will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = edges\n",
    "outline[edges != 0] = 1\n",
    "plt.imshow(outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Let's combine the above into a reusable function, `outline_img`. Please fix the FIXMEs below. Click the `...` for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g8TppOSuUPdN"
   },
   "outputs": [],
   "source": [
    "def outline_img(img):\n",
    "    threshold = 0.25\n",
    "    new_img = img.clone()\n",
    "    new_img[new_img > FIXME] = 1\n",
    "    new_img[new_img <= FIXME] = 0\n",
    "    edges_x = F.FIXME(new_img, Gx, stride=1, padding=1)\n",
    "    edges_y = F.FIXME(new_img, Gy, stride=1, padding=1)\n",
    "    edges = edges_x + edges_y\n",
    "    edges[edges != FIXME] = 1\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline_img(img):\n",
    "    threshold = 0.25\n",
    "    new_img = img.clone()\n",
    "    new_img[new_img > threshold] = 1\n",
    "    new_img[new_img <= threshold] = 0\n",
    "    edges_x = F.conv2d(new_img, Gx, stride=1, padding=1)\n",
    "    edges_y = F.conv2d(new_img, Gy, stride=1, padding=1)\n",
    "    edges = edges_x + edges_y\n",
    "    edges[edges != 0] = 1\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_wo0WS0X-k0"
   },
   "source": [
    "We can verify `outline_img` function works by running it on a batch of data. Let's modify our `show_images` function into a `show_outlined_images` function by adding our image outliner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "NMUw6V12RaK4",
    "outputId": "aaa5854f-36e3-413a-90d6-501519348d37"
   },
   "outputs": [],
   "source": [
    "def show_outlined_images(dataset, num_samples=10):\n",
    "    plt.figure(figsize=(16, 1))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_samples:\n",
    "            return\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        img = img[0][None, :, :, :]\n",
    "        img = torch.squeeze(outline_img(img))\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "\n",
    "show_outlined_images(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uPbIaTBeyWt",
    "outputId": "38c77043-580f-486e-c089-7b872f732fe6"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "def load_fashionMNIST(data_transform, train=True):\n",
    "    return torchvision.datasets.FashionMNIST(\n",
    "        \"./\",\n",
    "        download=True,\n",
    "        train=train,\n",
    "        transform=data_transform,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_transformed_fashionMNIST():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    train_set = load_fashionMNIST(data_transform, train=True)\n",
    "    test_set = load_fashionMNIST(data_transform, train=False)\n",
    "    return train_set, test_set\n",
    "\n",
    "train_set, valid_set = load_transformed_fashionMNIST()\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJDMeV0aY8Wr"
   },
   "source": [
    "## 2.2 Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpzH_qZgY_os"
   },
   "source": [
    "We can use vector embeddings to compare an outline with an article of clothing. One way to think of these embeddings is as geometric rays. We can use the angle between these rays to measure how similar they are. Try changing `x1`, `x2`, `y1`, and `y2` in the code block below to see what the cosine similarity is between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "DRfcIfskcMr9",
    "outputId": "a5ceaded-e546-40bb-dbc5-c154f7796cf1"
   },
   "outputs": [],
   "source": [
    "x1, y1 = [0, 1] # Change me\n",
    "x2, y2 = [1, 0] # Change me\n",
    "\n",
    "p1 = [x1, y1]\n",
    "p2 = [x2, y2]\n",
    "\n",
    "arrow_width = 0.05\n",
    "plt.axis('square')\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.arrow(0, 0, x1, y1, width=arrow_width, color=\"black\")\n",
    "plt.arrow(0, 0, x2, y2, width=arrow_width, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "cosine = np.dot(p1, p2) / (norm(p1) * norm(p2))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is not limited to two dimensions and can be used to compare two vectors of any dimensional length as long as they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSSGpbTdiZqW",
    "outputId": "220b9453-fde4-4716-f805-3b24ce00bb45"
   },
   "outputs": [],
   "source": [
    "p1 = [1, 8, 6, 7]\n",
    "p2 = [5, 3, 0, 9]\n",
    "\n",
    "cosine = np.dot(p1, p2) / (norm(p1) * norm(p2))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVyU7Ow5YuMO"
   },
   "source": [
    "## 2.3 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STIm3MzoLHfY"
   },
   "source": [
    "Before we can create a contrastive pre-training model, we should define a model to create embeddings for each of our data types. Any type of model could work as long as the output is a vector. Since both our our inputs are images, we can use a convolutional neural network. In this case, the final layer is a `Linear` layer. The size of that layer determines the size of the output embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XItWRTf5WwDr"
   },
   "outputs": [],
   "source": [
    "class ImgEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_ch, img_size, down_ch_1=32, down_ch_2=64, embed_dim=10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, down_ch_1, kernel_size, stride, padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(down_ch_1, down_ch_2, kernel_size, stride, padding),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(down_ch_2 * (img_size // 4) ** 2, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        emb = self.dense_emb(conv)\n",
    "        return F.normalize(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a model to compare the two vectors from our embedding models. A couple of important operations are [repeat_interleave](https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html) and [repeat](https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html). Since we are comparing a batch at a time, this allows each vector from one embedder to be compared against each vector from the other embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "repeat_x = x.repeat(3)\n",
    "repeat_interleave = x.repeat_interleave(3)\n",
    "print(\"repeat: \", repeat_x)\n",
    "print(\"repeat_interleave: \", repeat_interleave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then calculate the `CosineSimilarity` of each combination of vectors. Then, we will use [torch.unflatten](https://pytorch.org/docs/stable/generated/torch.unflatten.html) to turn the result into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.unflatten(repeat_x, 0, (3, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.unflatten(repeat_interleave, 0, (3, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together in the `ContrastivePretraining` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IeTscbuSQkeI"
   },
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self, in_ch, img_size, embed_dim=10):\n",
    "        super().__init__()\n",
    "        self.baseImgEmbedder = ImgEmbedder(in_ch, img_size, down_ch_2=128, embed_dim=embed_dim)\n",
    "        self.outlineEmbedder = ImgEmbedder(in_ch, img_size, embed_dim=embed_dim)\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, base_imgs, outlined_imgs):\n",
    "        base_emb = self.baseImgEmbedder(base_imgs)\n",
    "        outline_emb = self.outlineEmbedder(outlined_imgs)\n",
    "\n",
    "        repeated_base_emb = base_emb.repeat_interleave(len(outline_emb), dim=0)\n",
    "        repeated_outline_emb = outline_emb.repeat(len(base_emb), 1)\n",
    "\n",
    "        similarity = self.cos(repeated_base_emb, repeated_outline_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_base = similarity\n",
    "        logits_per_outline = similarity.T\n",
    "        return logits_per_base, logits_per_outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to define the model. We chose to use two [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) functions to make the math a little clearer in our training loop. Since we're comparing two data types, our `total_loss` will be the average of the loss for the outlines and the loss for the original images. Since [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) compares against the index of the target value, we'll use `arange` to generate the `ground_truth` indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jnJnQOGteALI"
   },
   "outputs": [],
   "source": [
    "model = ContrastivePretraining(1, 28)\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "loss_base = nn.CrossEntropyLoss()\n",
    "loss_outline = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long)\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to begin training. As the model is training, try to keep an eye on the output matrix. Do the values along the diagonal get closer to 1, and do the other values get closer to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "psvEW18yeUAT",
    "outputId": "9ed6db96-fcd3-4f0d-f140-ba3c1ce61dcd"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = batch[0]\n",
    "        outlines = outline_img(images)\n",
    "        logits_per_base, logits_per_outline = model(images, outlines)\n",
    "        total_loss = (loss_base(logits_per_base, ground_truth) + loss_outline(logits_per_outline, ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 1 == 0 and step % 2000 == 0:\n",
    "            print(f\"Train Epoch {epoch} | Step {step:03d} Loss: {total_loss.item()} \")\n",
    "            print(\"Similarity:\")\n",
    "            print(logits_per_base)\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        images = batch[0]\n",
    "        outlines = outline_img(images)\n",
    "        logits_per_base, logits_per_outline = model(images, outlines)\n",
    "        total_loss = (loss_base(logits_per_base, ground_truth) + loss_outline(logits_per_outline, ground_truth))/2\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        valid_loss += total_loss.item()\n",
    "\n",
    "    print(f\"Valid Loss: {valid_loss / step} \")\n",
    "    print(\"Similarity:\")\n",
    "    print(logits_per_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Vector Lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, let's put it to use! We've provided an image at `images/my_outline.png`, but feel free to upload your own sketch to see how it compares. Once we've identified the image we'd like to search with, we'll convert it from 3 color channels to 1 by using `gray_mode`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "7dTRnlwYCtHb",
    "outputId": "c632b912-f7e3-45ac-8684-c843ad5dca26"
   },
   "outputs": [],
   "source": [
    "gray_mode = torchvision.io.ImageReadMode.GRAY\n",
    "my_outline = torchvision.io.read_image(\"images/my_outline.png\", gray_mode)\n",
    "my_outline = my_outline.float() / 255\n",
    "my_outline.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XUSpEozNHsP"
   },
   "outputs": [],
   "source": [
    "plt.imshow(my_outline[0], cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll wrap it in a batch so it can be run through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_-P66mTDawc"
   },
   "outputs": [],
   "source": [
    "my_batched_outline = my_outline[None,:,:,:]\n",
    "my_batched_outline.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're searching with an outline, we'll use the `outlineEmbedder` to get its corresponding embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5en7LfcDlrk"
   },
   "outputs": [],
   "source": [
    "out_emb = model.outlineEmbedder(my_batched_outline)\n",
    "out_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn our `train_set` into a vector database by running the images through the `baseImgEmbedder`. Then, we can calculate the `cos`ine similarity between each image embedding and the embedding for our outline.`\n",
    "\n",
    "Since the dataset is so large, it would be challenging to run all the images through the model at once, so we'll a batch at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "best_img = None\n",
    "compare_batch_size = 5000\n",
    "\n",
    "cos = nn.CosineSimilarity()\n",
    "repeated_out_emb = out_emb.repeat(len(out_emb), 1)\n",
    "compare_dataloader = DataLoader(train_set, batch_size=compare_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2yHT8-bFJg7"
   },
   "outputs": [],
   "source": [
    "for step, batch in enumerate(compare_dataloader):\n",
    "    images = batch[0]\n",
    "    img_embs = model.baseImgEmbedder(images)\n",
    "    scores = cos(img_embs, repeated_out_emb)\n",
    "    best_idx = torch.argmax(scores)\n",
    "    batch_best_score = scores[best_idx]\n",
    "    print(\"Step:\", step, \"| Batch Best Score:\", batch_best_score.item())\n",
    "    if batch_best_score.item() > best_score:\n",
    "        best_score = batch_best_score.item()\n",
    "        best_img = images[best_idx]\n",
    "print(\"Best Score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the lucky winner. How close does the image match the outline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93uA_lIkLGnL"
   },
   "outputs": [],
   "source": [
    "plt.imshow(best_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo7WW4R3MfKY"
   },
   "outputs": [],
   "source": [
    "compare_outline = outline_img(best_img)\n",
    "plt.imshow(compare_outline[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Zd3gbP_Mnpi"
   },
   "outputs": [],
   "source": [
    "plt.imshow(my_outline[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on finishing this lab. Contrastive pre-training is a powerful technique that allows us to substitute one data type for another. Not only can it be used for searching through vector databases, but other models trained on the embeddings of a contrastive pre-training model allows us to perform inference with either data type.\n",
    "\n",
    "By now, the experiments from the previous lab are likely complete. Please return to [02a_Intermediate_Fusion](02a_Intermediate_Fusion.ipynb) to see the results. Please also run the cell below to free up resources for future labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
