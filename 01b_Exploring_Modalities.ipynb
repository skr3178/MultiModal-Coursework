{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70bed1e-e605-4d43-b378-785564c601fd",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d39bf-857c-4f73-8ad9-ce6ee0948194",
   "metadata": {},
   "source": [
    "# 1b. Exploring Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c984f-b57c-4ade-a58e-55c329348c4a",
   "metadata": {},
   "source": [
    "In the last lab, we learned about two different modes of data: LiDAR data and RGB data. In this lab, we will explore other kinds of data. Multimodal models is a large field of study, and getting practice with a variety of data types will make it easier when facing a new data type.\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Explore audio data\n",
    "* Explore CT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b7503-0ddf-41fd-a52e-b3b299a49f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy import fft\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.animation as animation\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "\n",
    "import IPython\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e157b-c6a4-4ec9-a16c-affc197a3c56",
   "metadata": {},
   "source": [
    "## 1.1 Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977239db-e861-4db4-b8b0-48e029f734ae",
   "metadata": {},
   "source": [
    "The first type of data we will explore is audio data. Interestingly, we can use the same neural network techniques we use to analyze images to analyze audio. Let's see how. We'll use SciPy's [wavefile.read](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html) function to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5e228-8679-4d94-8383-7b401f8fc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate, data = wavfile.read('data/cat_example-1.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a343ae-08a0-4e7e-a7d2-135b20a72551",
   "metadata": {},
   "source": [
    "The rate is how many samples per second there are in the audio file. Sound is an analog wave, the higher fidelity its discrete digital representation reflects the original. For example, the highest frequency that can be reliably captured is half the sampling frequency (Nyquist limit). Learn more about it [here](https://en.wikipedia.org/wiki/Sampling_(signal_processing))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5352fde-253e-4bdd-a4b8-f57ff213c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate  # Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810284c3-c409-4aaa-a442-46ddb71dc9ea",
   "metadata": {},
   "source": [
    "Our `data` has two dimensions. The first is the total number of samples taken. The second is the number of channels. Since our sample file is stereo, it has `2` channels: one for each ear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c8e71-675c-43d4-ab2b-bbab95235490",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db3fed-d23e-4816-9356-68ef4fdad069",
   "metadata": {},
   "source": [
    "The [WAV file format](https://en.wikipedia.org/wiki/WAV) captures a sound wave's amplitude. Because we have an amplitude sampled at a fixed interval of time, we can calculate the frequencies found in the audio by performing a [Fast Fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) on small windows of data. Once we have the frequencies, we can create a [Spectrogram](https://en.wikipedia.org/wiki/Spectrogram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7f219-0e51-497a-9160-e04a1fd43831",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.specgram(data[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d709f-3566-416a-8a73-7b55b90755f2",
   "metadata": {},
   "source": [
    "Here, the horizontal axis represents time, the vertical axis represents frequency, and brightness represents the frequency's amplitude. Because we have an image, we can now use a convolutional neural network to analyze it.\n",
    "\n",
    "By the way, we can write a NumPy array into a `.wav` file using [wavfile.write](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html). If you're in a classroom setting, please lower your volume to be respectful of the people around you. Can you recognize the sound? The answer is in the `...` below. Trying changing the `speed` value below to see how it changes the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1b31b-10b4-4431-b1a9-6ab45b1ad1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 1\n",
    "new_rate = int(rate * speed)\n",
    "wavfile.write('data/temp.wav', new_rate, data)\n",
    "IPython.display.Audio('data/temp.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa22c3-d3bf-44c3-96a1-2516ecadc228",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Answer:\n",
    "It's a cat purring. Thanks to [Mysid](https://en.wikipedia.org/wiki/File:Purr.ogg) for making this available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01988d83-ba92-4ce4-9143-ee4020ee67c7",
   "metadata": {},
   "source": [
    "## 1.2 CT Scan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6a779-eb63-424f-8e9d-7b3d9dd7c636",
   "metadata": {},
   "source": [
    "The second type of data we will explore is CT (often pronounced \"cat\") data. [CT scans](https://www.mayoclinic.org/tests-procedures/ct-scan/about/pac-20393675) are an imaging tool created by repeatedly taking X-rays at different positions of a body. This kind of data is often represented using the [NIfTI](https://afni.nimh.nih.gov/pub/dist/doc/htmldoc/nifti/format.html) file format. The [NiBabel](https://github.com/nipy/nibabel/tree/master) library provides useful tools to view this data. Let's start by looking at an example file `header`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424c99c-3f27-4bb2-b6cd-b1abe76a0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/cat_example-2.nii\"\n",
    "\n",
    "ct_file = nib.load(path)\n",
    "print(ct_file.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c01bf1-e12b-4f6f-bde2-b46233624cea",
   "metadata": {},
   "source": [
    "The header contains information such as the position offset, the data type, and the size of the data array. Learn more about it [here](https://brainder.org/2012/09/23/the-nifti-file-format/).\n",
    "\n",
    "The imaging data itself is a 3-dimensional matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02432687-d8e4-4e5f-9876-148d431a240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_matrix = ct_file.get_fdata()\n",
    "ct_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7ee9d-96a9-4284-8c38-0cd652bb4933",
   "metadata": {},
   "source": [
    "Let's take a slice of data and view it with [Matplotlib](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10607cc-de82-4598-bb08-893a2f78480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ct_matrix[:,:,0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84266a1-8807-47ae-af6d-ec1f08065dab",
   "metadata": {},
   "source": [
    "It may be hard to tell, but this is a cross-section of a person's torso. The \"C\"-shaped object in the image is the bed of the CT machine, so let's rotate this person so they're comfortably laying right side up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa773c-c6d2-41f5-9d81-e0deb1e7bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_matrix90 = np.rot90(ct_matrix, k=1, axes=(0, 1))\n",
    "plt.imshow(ct_matrix90[:,:,0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64575f-482f-437c-a8c6-07ea1d3fe7ff",
   "metadata": {},
   "source": [
    "Much better! Next, we can animate each frame of the CT Scan so we can better identify medical anomalies. Let's define a function, `animate_ct_scan` to loop through each element along a specified data axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d792cd-adef-4969-b091-10beb2f26ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_ct_scan(axis):\n",
    "    # Generate some sample data for the frames\n",
    "    frames = ct_matrix90.shape[axis]\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Define the animation function\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        ct_slice = np.take(ct_matrix90, frame, axis=axis)\n",
    "        ax.imshow(ct_slice, cmap=\"Greys\")\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=75)\n",
    "\n",
    "    # Display the animation in Colab\n",
    "    return HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72e9a7-1ba8-41aa-8e54-abd5f61bd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_ct_scan(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708fcec-561a-494c-ba3b-0ed48d39206d",
   "metadata": {},
   "source": [
    "This animation moves from the top of this person's head down to the base of their spine. The two white areas that appear are their lungs.\n",
    "\n",
    "Because the data is 3D, we can move along a different axis. The below will traverse the CT Scan from the left arm to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde5908-6440-49a9-9a94-7d4ca3f48ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_ct_scan(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc8d38-0aeb-4686-a1d0-998d4c17e3fb",
   "metadata": {},
   "source": [
    "To complete the set, let's see the data from top to bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a76bee-cae1-4ab3-8b1f-ad4778df59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_ct_scan(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23833542-f472-493e-b26e-9157fad3bfc2",
   "metadata": {},
   "source": [
    "Biomedical analysis is one of the largest applications of multi-modal models. Because of their 3-dimensional shape, CT scan data is often analyzed with a neural network architecture called a [U-Net](https://arxiv.org/abs/1505.04597). These U-Nets are an evolution of convolutional neural networks and are used to highlight potentially anomalous regions in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d26de-7595-4a6a-808a-36f556daa53b",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e064a-c757-4478-8954-787eb37fa390",
   "metadata": {},
   "source": [
    "Congratulations on finishing this lab! There is a lot of interesting data in the world, and we hope this has piqued your interest in learning more. Hopefully by now, the experiments from the previous lab are now complete. Please go back and check it out. Before you do, please run the cell below to free up computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784217ff-6d7b-4daf-8cef-6ebe0cce36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b3d28-9f1e-48dd-925c-1f3f0f8c4089",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
