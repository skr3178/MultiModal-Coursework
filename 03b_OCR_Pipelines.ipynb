{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb89b16-2072-42e0-99de-44364bb68cb0",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688cd9a-5546-4156-a5c5-5ca735b7a72a",
   "metadata": {},
   "source": [
    "# 3b. OCR Pipelines\n",
    "\n",
    "Today, many large language model (LLM) systems can derive insights from various media such as news articles and academic papers. Both of these sources contain multimodal data such as text, figures, and tables. This tutorial provides a step-by-step guide showing how to isolate different modalities from multimodal PDF documents. The process of analyzing text in images is called Optical Character Recognition, or OCR. Let's create a multimodal pipeline with OCR and other PDF data extraction techniques to be used in a pipeline for LLM models. When an LLM retrieves information from such a pipeline, it's called retrieval augmented generation, or RAG.\n",
    "\n",
    "<img src=\"images/DLI_extract.png\" width=\"600\">\n",
    "\n",
    "In this tutorial, we will be using the open-source [unstructured](https://github.com/Unstructured-IO/unstructured) library for pre-processing images, tables and text from PDF documents.\n",
    "\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Explore Text Chunking\n",
    "* Explore Table Extraction\n",
    "* Explore Images/Plots Extraction\n",
    "* Explore Identifying page elements with YOLOX-L\n",
    "* Create an End-to-End Multimodal Data Extraction Pipeline\n",
    "\n",
    "Let's load some libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc19ae7-71d8-4a21-a3db-1672cba2a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import ipyplot\n",
    "import requests\n",
    "import base64\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image as IPyImage\n",
    "from IPython.display import IFrame\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "IMG_DIR = \"ocr/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cd5fe-856a-4463-94f3-42333376ab1c",
   "metadata": {},
   "source": [
    "## 3.1 Setup\n",
    "\n",
    "In this example, we will use the NVIDIA GB200 NVL72 white paper. It contains tables, plots, images and text description. Run the cell below to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acbe7b-795b-4aae-8ca3-c2b5883c3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/grace-blackwell-datasheet.pdf\"\n",
    "IFrame(filename, width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95dcb6-df09-4489-918f-9ff033438c45",
   "metadata": {},
   "source": [
    "Let's segment the PDF document using unstructured's `partition_pdf` function and extract the text. Optical Character Recognition (OCR) will be used on images when the text is not already available in the PDF format. This library relies on the [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3228b-7afc-4c7b-bde9-342c83a21d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Text, Image, Table, CompositeElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43946c4-ec43-4cc5-a660-b5fab1d7ac2a",
   "metadata": {},
   "source": [
    "We can use `partition_pdf` directly on the pdf file to extract useful elements. Each element is a section of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd51953-2005-44b5-9377-cad74c8e6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition_pdf(filename)\n",
    "print(\"Retrieved elements: \", len(elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c3600-c27d-4619-bbe8-4ab1ef5c73eb",
   "metadata": {},
   "source": [
    "To get a sense of what these elements look like, let's print each element and its corresponding page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771a2b9-e553-4e7a-93b0-da83bc48a3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for el in elements:\n",
    "    if isinstance(el, Text):\n",
    "        print(\"page \", el.metadata.page_number)\n",
    "        print(el.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13cc543-3956-4f30-89d7-7e9d8b2ae58d",
   "metadata": {},
   "source": [
    "## 3.2 Explore Text Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1841d517-3790-4dea-923c-1e6f9abed112",
   "metadata": {},
   "source": [
    "Text chunking involves transforming the text data from the PDF into smaller chunks that can be embedded and stored for later retrieval. The library provides a naive chunking strategy based on chunk size parameters (max characters). This method is purely syntactic and can limit the performance of the RAG pipeline if the chunks are not semantically distinct enough.  \n",
    "\n",
    "There is a more granular chunking strategy `by_title` where section boundaries are preserved and we can split the text into more semantically distinct chunks. Let's check the granular chunking strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159eded-95e5-48c9-ba8d-ed013f27acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "granular_elements = partition_pdf(\n",
    "    filename,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=2000,\n",
    "    new_after_n_chars=1800,\n",
    "    combine_text_under_n_chars=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62290bc9-896f-4fe1-85ee-baab7f087c28",
   "metadata": {},
   "source": [
    "First, let's check how many text sections are detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e6a4e-a701-493e-8ed9-9f8d113cfadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many text section detected\n",
    "print(\"Retrieved granular elements: \", len(granular_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b34067-9a00-4993-ac83-6eeb05392896",
   "metadata": {},
   "source": [
    "Next, let's compare the first extracted section for both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447e81c-cde0-4ac2-9d5c-216e836f37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare extracted section 1 for both methods\n",
    "print(\"Naive chunking method: \\t\", elements[0].text.strip()[:200], end=\"\\n\\n\")\n",
    "print(\"Granular chunking method: \\t\", granular_elements[0].text.strip()[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115176-645b-4047-8892-3ce537657d42",
   "metadata": {},
   "source": [
    "The granular chunking method has extracted and grouped together the same sections of text from the PDF. This should help the retrieval process of our OCR pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db7aaf-48a4-4ba3-b6f8-1001430bf1c4",
   "metadata": {},
   "source": [
    "## 3.3 Explore Table Extraction\n",
    "The [unstructured](https://unstructured.io/) library uses the transformer-based object detection model [microsoft/table-transformer-structure-recognition](https://huggingface.co/microsoft/table-transformer-structure-recognition) to extract tables from PDFs.\n",
    "\n",
    "To enable this, we will need to specify `infer_table_structure` as True.\n",
    "\n",
    "<div class=\"alert alert-warning\">Running the next cell can take about ~22s</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27825f6-e5a8-4bf6-838d-4fbd52df7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "table_text_elements = partition_pdf(\n",
    "    filename, infer_table_structure=True, strategy=\"hi_res\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2e1d0-3b5f-461a-88a4-e153e6d90dbe",
   "metadata": {},
   "source": [
    "The `text_as_html` element metadata attribute allows us to directly view the elements corresponding HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45079986-4c02-435c-8633-132ceceb9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in table_text_elements:\n",
    "    if type(el) == Table:\n",
    "        print(\"Table\")\n",
    "        print(el.metadata.text_as_html) \n",
    "        el_html = el.metadata.text_as_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61a22c-3997-4409-bd5c-e98aed29028c",
   "metadata": {},
   "source": [
    "We can then convert this information into html data which can then be ingested by the [LangChain](https://python.langchain.com/docs/introduction/) framework, a popular framework used to make LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df45de-9558-4015-a2b9-920da87c787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "table_elements = [\n",
    "    Document(\n",
    "        page_content=e.metadata.text_as_html,\n",
    "        metadata={\"filename\": e.metadata.filename, \"source_type\": \"table_html\"},\n",
    "    )\n",
    "    for e in table_text_elements\n",
    "    if type(e) == Table\n",
    "]\n",
    "\n",
    "# check first element\n",
    "table_elements[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9db639-3d2c-4e54-aff5-b43f9bedfe75",
   "metadata": {},
   "source": [
    "Since this content is HTML, we can display it within this Jupiter notebook like we could on a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe98548-de7c-434d-a472-3c3ba2b01246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the table as HTML\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import display\n",
    "display(HTML(el_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab217e65-a194-471b-b124-8275aba394d7",
   "metadata": {},
   "source": [
    "Most of the table has been accurately extracted, but there are some missing rows. The table extraction performance could be improved using techniques such as converting the PDF to markdown format and using the markdown representation of the table for retrieval, or by extracting the table as an image, which we will look at next.\n",
    "\n",
    "## 3.4 Explore Images/Plots Extraction\n",
    "\n",
    "To extract images from a PDF, we can set `extract_images_in_pdf=True`. The `unstructured` library uses the [YOLOX](https://arxiv.org/pdf/2107.08430v2) model to detect images. YOLOX is an anchor-free version of YOLO (You Only Look Once), a one-shot object detection model series from Megvii Technology.\n",
    "\n",
    "```\n",
    "elements = partition(filename=filename,\n",
    "                     strategy=\"hi-res\",\n",
    "                     hi_res_model_name=\"yolox\")\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-warning\">Running the next cell can take about 12s.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dd846-d8f2-4ad9-b4aa-ab858e0e241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"UNSTRUCTURED_HI_RES_MODEL_NAME\"] = \"yolox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028feb01-9566-47cb-9424-498c25c70adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text_elements = partition_pdf(\n",
    "    filename=filename,                 \n",
    "    strategy=\"hi_res\",    \n",
    "    hi_res_model_name=\"yolox\",\n",
    "    extract_images_in_pdf=True,                            \n",
    "    extract_image_block_types=[\"Image\"],          \n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=IMG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a4eaa-e5d5-456b-ab4b-55ed4af611b2",
   "metadata": {},
   "source": [
    "Let's check the outputs to verify images were extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bb340-2755-46a1-a62f-21397129391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [e.metadata.image_path for e in image_text_elements if type(e) == Image]\n",
    "print(\"list of extracted images:\\n\", images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4e9b4-a850-467c-b0dc-d116fec0ab23",
   "metadata": {},
   "source": [
    "We can then create a function to visualize the extracted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ee0a5-2d75-403f-ae40-dda22a8741bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_from_folder(folder_path=None, image_files=None):\n",
    "    if image_files == None:\n",
    "        image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "    if image_files:\n",
    "        ipyplot.plot_images(image_files, max_images=20, img_width=200, show_url=False)\n",
    "    else:\n",
    "        print(\"No images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7a840-bd5f-4671-8740-f4082c68c9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_images_from_folder(image_files=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe7a241-cc62-4900-b10b-35ee0f942040",
   "metadata": {},
   "source": [
    "We can also process tables using the YOLOX model. The tables will be extracted as images.\n",
    "\n",
    "Use `extract_image_block_types = [\"Table\"]`\n",
    "<div class=\"alert alert-warning\">Running the next cell can take about 16s</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769223f-68e5-40f2-8cba-0773d619859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_table_text_elements = partition_pdf(\n",
    "    filename=filename,                 \n",
    "    strategy=\"hi_res\",    \n",
    "    hi_res_model_name=\"yolox\",\n",
    "    infer_table_structure=True,\n",
    "    extract_images_in_pdf=True,                            \n",
    "    extract_image_block_types=[\"Image\", \"Table\"],          \n",
    "    extract_image_block_to_payload=False,                  \n",
    "    extract_image_block_output_dir=IMG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcfe15-ac2d-45a6-943e-8d06205fdfcf",
   "metadata": {},
   "source": [
    "`unstructured` has a `elements_to_json` function that we can use to convert the table information into a [JSON](https://www.json.org/json-en.html) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499811be-21f6-4063-98a5-d187553f0b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unstructured.staging.base import elements_to_json\n",
    "tables = [e for e in image_table_text_elements if type(e) == Table]\n",
    "elements_to_json(image_table_text_elements, filename=f\"{filename}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d4653-a579-4ae2-a2d7-c29f513e8374",
   "metadata": {},
   "source": [
    "While the table information was extracted, it's hard to read in this format. We can instead view the table as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c2ac3-4aec-4d88-a0b7-2d1cde0035a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_images_from_folder(folder_path=IMG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642af8d-d419-4cdb-91a6-3e243f9bc1ac",
   "metadata": {},
   "source": [
    "## 3.5 Identifying Page Elements with NV-YOLOX\n",
    "\n",
    "YOLOX-L is a version of YOLOX fine-tuned on 26,000 images from the Digital Corpora dataset, with annotations from Azure AI Document Intelligence. The model is trained to detect **tables**, **charts** and **titles** in documents. A chart is defined as a bar chart, line chart, or pie chart. Titles can be page titles, section titles, or table/chart titles.\n",
    "\n",
    "Let's use the [nvidia/nv-yolox-page-elements-v1](https://build.nvidia.com/nvidia/nv-yolox-page-elements-v1) model to locate the bounding boxes of elements in a [PNG](https://en.wikipedia.org/wiki/PNG) file. The bounding box tells us the location of an element within an image. Let's use the below image as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1124bc-3494-487f-9c92-6da77f4fee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = \"images/nvidia-tensorrt-llm-enhancements-deliver-massive-large-language-model-speedups-on-nvidia-h200.png\"\n",
    "IPyImage(filename=image_filename, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728f010-5dcb-4b76-9a83-a54c35f4d9ed",
   "metadata": {},
   "source": [
    "We can create a function that calls the YOLOX model API endpoint and returns the page elements with their precise locations on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6a62c-29df-4108-b578-0d44b1402a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolox(filename):\n",
    "    invoke_url = \"http://ngc-client:9505/v1/cv/nvidia/nv-yolox-page-elements-v1\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode()\n",
    "    assert len(image_b64) < 250_000, \"To upload larger images, use the assets API (see docs)\"\n",
    "    \n",
    "    headers = {\n",
    "       \"Authorization\": \"Bearer $API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC\",\n",
    "       \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "      \"input\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"url\": f\"data:image/png;base64,{image_b64}\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "    print(response)\n",
    "    bounding_boxes = response.json()['data'][0]['bounding_boxes']\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8e962-9fa0-4e89-9a9d-ede8e7172f55",
   "metadata": {},
   "source": [
    "Time to test it out. Let's use `run_yolox` on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf965d23-ad91-4da0-a6a1-cadca9a49be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = run_yolox(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb64413-402a-4e08-a5d4-738d33fdc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab15de80-3335-4473-afea-a2769bcd16c7",
   "metadata": {},
   "source": [
    "Here, we can see the model identified 3 elements: a table, a chart, and a title.\n",
    "\n",
    "Let's visualize those bounding boxes around the elements that were identified by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efbf5c-42c2-4418-a6d7-c73f112b2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "def plot_bounding_boxes(image, bounding_boxes):\n",
    "    annotated_image = image.copy()\n",
    "    height, width = image.shape[:2]\n",
    "    color = (128, 128, 128)\n",
    "    labels = ['table', 'chart', 'title']   \n",
    "    for l in labels:\n",
    "        # check if label is identified\n",
    "        if l not in bounding_boxes:\n",
    "            continue\n",
    "        T = bounding_boxes[l]\n",
    "        for t in T:\n",
    "            x_min = int(width * t['x_min'])\n",
    "            y_min = int(height * t['y_min'])\n",
    "            x_max = int(width * t['x_max'])\n",
    "            y_max = int(height * t['y_max'])\n",
    "            label = f\"{l} {t['confidence']:.2f}\"\n",
    "            color = (0, 255, 0)  # Green color for the box\n",
    "            cv2.rectangle(annotated_image, (x_min, y_min), (x_max, y_max), color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(annotated_image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_PLAIN, fontScale=1.5, color=color, thickness=1)\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6a7eb-eef7-4ae3-a4db-dc5b6d494681",
   "metadata": {},
   "source": [
    "We can draw the extracted bounding boxes and save them on a temporary image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e79ad9-7b2e-44e6-bfbf-40a15650ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_filename)\n",
    "annotated_image = plot_bounding_boxes(image, bounding_boxes)\n",
    "OUTPUT_IMAGE_PATH = \"ocr/outputs/yolox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66cbe2-b92c-4db8-b7c9-55c20b79fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(OUTPUT_IMAGE_PATH + '/test_annotated.jpg', annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be07e2-f4a1-4f25-b843-d270b311d957",
   "metadata": {},
   "source": [
    "Then, we can visualize the elements detected by `nv-yolox-page-elements-v1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73484dcf-8fe0-4206-924b-4afec785cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage(filename=OUTPUT_IMAGE_PATH + '/test_annotated.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb673c-99d7-4c9d-97a2-973d9c1ddc89",
   "metadata": {},
   "source": [
    "## 3.5.1 Exercise: Bounding Boxes\n",
    "\n",
    "Now let's try this with our original example NVIDIA GB200 NVL72 white paper from earlier.\n",
    "\n",
    "First we need to convert each page of the PDF to an image. There are a couple ways to do this. We could process the entire PDF in one go by using `convert_from_path`, which loads all pages into memory at once:\n",
    "\n",
    "```\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "pages = convert_from_path(filename, 500)\n",
    "for count, page in enumerate(pages):\n",
    "    page.save(f'pdf2image/out_{count}.jpg', 'JPEG')\n",
    "```\n",
    "\n",
    "However, this can consume significant memory, particularly for large PDFs and systems with limited RAM.\n",
    "\n",
    "Instead, we can process the data using batching so that only a portion of the PDF is loaded into memory at a time. The `CHUNK_SIZE` can be adjusted based on available system resources to make processing more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fb764-b341-4702-9360-0d3d36117cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import pdfinfo_from_path, convert_from_path\n",
    "\n",
    "filename = \"data/grace-blackwell-datasheet.pdf\"\n",
    "PATH_PDF2IMAGE = \"ocr/outputs/pdf2image\"\n",
    "\n",
    "CHUNK_SIZE = 20  # depends on RAM\n",
    "MAX_PAGES = pdfinfo_from_path(filename)[\"Pages\"]\n",
    "\n",
    "pages = []\n",
    "for page in range(1, MAX_PAGES, CHUNK_SIZE):\n",
    "    pages += convert_from_path(filename, first_page=page, last_page=page + CHUNK_SIZE - 1)\n",
    "\n",
    "for count, page in enumerate(pages):\n",
    "    page.save(f'{PATH_PDF2IMAGE}/test_{count}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1158c-6672-4c48-93aa-6ca49cfcbf27",
   "metadata": {},
   "source": [
    "Let's verify the image chunks were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462b2d3-a822-4892-97f0-97c57dd2cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $PATH_PDF2IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7495b4-86dd-4a75-955a-08b7b653a30f",
   "metadata": {},
   "source": [
    "Then, let's visualize the created chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dd2e4-9a90-4926-bc50-4c31fcceb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's visualize the pages\n",
    "plot_images_from_folder(folder_path=PATH_PDF2IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6e77c-e6fd-474f-bcef-d6b3a458e2f4",
   "metadata": {},
   "source": [
    "We can take one of these pages and compute the bounding boxes of the page elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c390b-eeb0-424f-8b29-045180c91a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_boxes = run_yolox(PATH_PDF2IMAGE + \"/test_2.jpg\")\n",
    "image = cv2.imread(PATH_PDF2IMAGE + \"/test_2.jpg\")   \n",
    "annotated_image = plot_bounding_boxes(image, bounding_boxes)\n",
    "cv2.imwrite(OUTPUT_IMAGE_PATH + '/annotated_test_1.jpg', annotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8720b0-8e75-490d-a374-d604cbabe15d",
   "metadata": {},
   "source": [
    "And we can visualize the identified titles and chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d15e4f-548a-4a91-b638-0bdbc9e3ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=OUTPUT_IMAGE_PATH + '/annotated_test_1.jpg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d883c-8fa2-442e-a8e0-1f6122b7ef15",
   "metadata": {},
   "source": [
    "If you encounter any issues, refer to the prepared folder `TMP_PATH_PDF2IMAGE` by running the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cd92f-21a9-4781-97a9-76362249fe8d",
   "metadata": {},
   "source": [
    "## 3.6 End-to-End Multimodal Data Extraction\n",
    "\n",
    "Now we've looked at several different methods for pre-processing multimodal PDFs, we can try another end-to-end example.\n",
    "\n",
    "Here we have a technical paper [ChipNemo](https://arxiv.org/pdf/2311.00176):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c44b8c-7182-4566-8e17-faca7cd1505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"ocr/pdfs/ChipNeMo.pdf\", width=800, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41bdb5-d012-4935-bd5d-7b913f62958b",
   "metadata": {},
   "source": [
    "The PDF contains a mixture of text under different sections, figures, tables, and graphs.\n",
    "\n",
    "If we have a folder of PDFs, we will need to define a method to process each one. We can process the PDFs with different chunking strategies using `partition_pdf`, such as `by_title` or `high_res`. The `by_title` strategy will be used to extract the raw text and table elements from the paper and store them in HTML format. The `high_res` strategy is used to extract the figures and tables as images. Let's look at `by_title` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f31aeb-a7de-4b80-b6c5-44821e64e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionPipeline:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.pdf_filenames = self.get_pdf_filenames_from_folder()        \n",
    "\n",
    "    def get_pdf_filenames_from_folder(self):\n",
    "        return list(Path(self.folder_path).glob(\"*.pdf\"))\n",
    "\n",
    "    def load_and_split_documents(self, folder_path):\n",
    "        document_chunks = []\n",
    "        for pdf_filename in self.pdf_filenames:\n",
    "            print(\"processing document: \", pdf_filename)\n",
    "            raw_pdf_elements = partition_pdf(\n",
    "                pdf_filename, \n",
    "                infer_table_structure=True, \n",
    "                strategy='hi_res'\n",
    "            )\n",
    "            Text_elements = [Document(page_content=e.text.strip(), metadata={\"filename\": e.metadata.filename, \"source_type\": \"text\"}) for e in raw_pdf_elements if type(e) == Text or CompositeElement]\n",
    "            print(\"Number of Detected Text elements: \", len(Text_elements))\n",
    "\n",
    "            # Tables\n",
    "            Table_elements = [Document(page_content=e.metadata.text_as_html, metadata={\"filename\": e.metadata.filename, \"source_type\": \"table_html\"}) for e in raw_pdf_elements if type(e) == Table]\n",
    "            print(\"Number of Detected HTML Tables: \", len(Table_elements))\n",
    "        return Text_elements, Table_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed178619-4e78-4872-ab4a-1bbbb5bced70",
   "metadata": {},
   "source": [
    "Let's now call the extraction function. This will extract text and tables (in HTML format) from the PDFs in the folder using the `by_title` strategy. We've specified parameters such as `max_characters`, `new_after_n_chars`, and `combine_text_under_n_chars` to control the chunk sizes of the text elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d40ae-1228-4bcd-8282-aa764644b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FOLDER_PATH = \"data\"\n",
    "Extraction_pipeline = ExtractionPipeline(folder_path=PDF_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ec964-42c7-4200-9d8e-85ec4cd3e1fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Running the next cell can take about 3mn.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349c531-95ff-493c-8fee-de85c8c23437",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# 2min 42s if you use default, 2s if you use fast but you get only text (no tabular data)\n",
    "Text_elements, Table_elements = Extraction_pipeline.load_and_split_documents(folder_path=PDF_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2601bb5-d575-41c1-af82-6b525a67b6fc",
   "metadata": {},
   "source": [
    "Let's verify the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcbe22-bda1-49fa-956e-5b81ba5210a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_elements[0].page_content, Text_elements[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6e9a2-797c-4e92-8f4d-b0dca866266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table_elements[0].page_content, Table_elements[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fcafa-ae51-4ced-a236-edb9c3cd349d",
   "metadata": {},
   "source": [
    "Finally, we can [pickle](https://docs.python.org/3/library/pickle.html) our extracted data to be stored in our hypothetical RAG database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79220a-b848-4a56-8bb7-b200f68e4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text and HTML tables \n",
    "import pickle\n",
    "\n",
    "PATH_DATA = \"ocr/outputs/data_dir_extracted\"\n",
    "\n",
    "with open(PATH_DATA + '/tables_db.pkl', 'wb') as fp:\n",
    "    pickle.dump(Table_elements, fp)\n",
    "\n",
    "with open(PATH_DATA + '/text_db.pkl', 'wb') as fp:\n",
    "    pickle.dump(Text_elements, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833099de-c788-4edd-9a7c-b9775c60d78a",
   "metadata": {},
   "source": [
    "After saving all the text and table information from the paper, we can look at the `high_res` chunking strategy for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4529189-9c3c-4168-a233-4552088a4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualExtractionPipeline:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.pdf_filenames = self.get_pdf_filenames_from_folder()        \n",
    "\n",
    "    def get_pdf_filenames_from_folder(self):\n",
    "        return list(Path(self.folder_path).glob(\"*.pdf\"))\n",
    "\n",
    "    def load_and_split_documents(self, folder_path):\n",
    "        for pdf_filename in self.pdf_filenames:\n",
    "            print(\"processing document: \", pdf_filename)\n",
    "            image_text_elements = partition_pdf(\n",
    "                filename=pdf_filename,                 \n",
    "                strategy=\"hi_res\",    \n",
    "                hi_res_model_name=\"yolox\",\n",
    "                extract_images_in_pdf=True,                            \n",
    "                extract_image_block_types=[\"Image\", \"Table\"],          \n",
    "                extract_image_block_to_payload=False,                  \n",
    "                extract_image_block_output_dir=\"ocr/figures\"\n",
    "            )\n",
    "        image_elements = [Document(page_content=e.metadata.image_path, metadata={\"filename\": e.metadata.filename, \"source_type\": \"image\"}) for e in image_text_elements if type(e) == Image]\n",
    "        #table_elements = [Document(page_content=e.metadata.image_path, metadata={\"filename\": e.metadata.filename, \"source_type\": \"image_table\"}) for e in image_text_elements if type(e) == Table]\n",
    "        return image_elements  # + table_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22415e8b-2d74-48f8-af02-613cd1bb0f50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Running the next cell can take about 2-3mn.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac3f57-ba97-4518-bb68-cd89effa2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Extraction_pipeline_visual = VisualExtractionPipeline(folder_path=PDF_FOLDER_PATH)\n",
    "image_elements = Extraction_pipeline_visual.load_and_split_documents(folder_path=PDF_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487a7f8-6844-427d-9f5b-5ea40479ba8a",
   "metadata": {},
   "source": [
    "Finally, let's see the images we have extracted from the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ca5fd-87c3-4b30-9e2f-5415da66fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [e.page_content for e in image_elements]\n",
    "plot_images_from_folder(image_files=images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c63e26c-ab9e-4f59-8c12-11011a2574ad",
   "metadata": {},
   "source": [
    "As before, we can save our data in our hypothetical database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cd32e-b1e6-4af6-99fd-758ae7b0db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text and HTML tables \n",
    "import pickle\n",
    "\n",
    "PATH_DATA = \"ocr/outputs/data_dir_extracted\"\n",
    "\n",
    "with open(PATH_DATA + '/images_db.pkl', 'wb') as fp:\n",
    "    pickle.dump(image_elements, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc518108-a8c3-4302-95a1-b12cdb2c4e94",
   "metadata": {},
   "source": [
    "## Next\n",
    "Congrats on making it to the end!\n",
    "\n",
    "In this notebook, we:\n",
    "- Pre-processed multimodal PDFs using the *Unstructured* library, which is the first step for creating a multimodal RAG pipeline, and explored various functions to extract text, tables, images, and charts from PDFs.\n",
    "- Explored NIMs components designed for multimodal content extraction to streamline the process.\n",
    "\n",
    "In the next section of the course, we will use a completed Video Search and Summary (VSS) orchestrated pipeline. Let's learn more about it in the upcoming slide deck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d920d-d2e8-427f-b77b-181178e55e87",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
