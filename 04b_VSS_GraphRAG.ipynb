{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b446c8-7764-4e06-9845-7f6af17f45a1",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24546b1-5b35-42f7-bbb2-5bc2d18170bc",
   "metadata": {},
   "source": [
    "# 4b. VSS Workshop Part 2: Q&A and Graph-RAG\n",
    "\n",
    "In the last lab, we learned how to use NVIDIA's [Video Search and Summarization AI Agent](https://build.nvidia.com/nvidia/video-search-and-summarization/blueprintcard) to summarize video. In this lab, we will explore how users can ask questions about video content to this AI agent.\n",
    "\n",
    "#### Learning Objectives:\n",
    "The goals of this notebook are to:\n",
    "- Exploring Q&A on videos through VSS REST APIs\n",
    "- Understanding Graph-RAG Components\n",
    "- Visualizing knowledge graph in Neo4J\n",
    "- Q&A with Vector-RAG (no Graph-RAG)\n",
    "\n",
    "## Q&A with VSS\n",
    "\n",
    "VSS supports Question-Answering (Q&A) functionality via **Vector-RAG** and **Graph-RAG**. Vector-RAG is the only supported method for live stream processing. And Graph-RAG is specifically designed for video-based queries.\n",
    "\n",
    "**Q&A with Vector-RAG:** Captions generated by the VLM, along with their embeddings, are stored in Milvus DB. Given a query, the top five most relevant chunks are retrieved, re-ranked using ```llama-3.2-nv-rerankqa-1b-v2```, and passed to a LLM to generate the final answer.\n",
    "\n",
    "**Q&A with Graph-RAG:** To capture the complex information produced by the VLM, a knowledge graph is built and stored during video ingestion. Use an LLM to convert the dense captions in a set of nodes, edges, and associated properties. This knowledge graph is stored in a graph database. Captions and embeddings, generated with ```llama-3.2-nv-embedqa-1b-v2```, are also linked to these entities. By using Graph-RAG techniques, an LLM can access this information to extract key insights for Q&A.\n",
    "\n",
    "![VSS CA-RAG Diagram](images/VSS_CA-RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33824692-b1a3-4929-af28-dae42b159aa7",
   "metadata": {},
   "source": [
    "## 4.1 Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00972be0-d94a-4d53-b216-91cb4a04f3b5",
   "metadata": {},
   "source": [
    "We will be using the same VSS server as the previous lab. Let's verify that it is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165e1d3-82c1-47a5-8a40-28cd22eefb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_video = \"data/warehouse.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb04a44-d2e2-4378-8f35-617d9b18e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"via-server\" #e.g., 0.0.0.0 or localhost\n",
    "port = \"8000\" #e.g., 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad262f9-0b9a-463c-b43e-1f2941739ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml\n",
    "\n",
    "vss_url = f\"http://{host}:{port}\"\n",
    "\n",
    "ready_check_url = f\"{vss_url}/health/ready\"\n",
    "response = requests.get(ready_check_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(f\"Service is ready.\")\n",
    "else:\n",
    "    print(f\"Service is not ready. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfaed7-c3ed-48ca-9ad6-ee16bd0f1523",
   "metadata": {},
   "source": [
    "## 4.2 Exploring Q&A on Videos\n",
    "\n",
    "Please refer to the previous lab for exploring all REST API endpoints. Below we will use REST APIs to upload a file, start video processing with chat enabled, and then try out a few questions.\n",
    "\n",
    "<!-- ![Warehouse Scene](images/warehouse.png) -->\n",
    "\n",
    "<video width=\"1000 \" height=\" \" \n",
    "       src=\"data/warehouse.mp4\"  \n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb732aa-237a-470b-9173-1e44d7a1ef45",
   "metadata": {},
   "source": [
    "### 4.2.1 Upload Video File\n",
    "\n",
    "Let's start by uploading a video and storing the file-id from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ec3dc-f14d-4610-ae30-05c7d8fe8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_url = f\"{vss_url}/files\"\n",
    "files = {'file': ('filename_with_extension', open(warehouse_video, 'rb'))}\n",
    "\n",
    "response = requests.post(upload_url, data={'purpose': 'vision', 'media_type': 'video'}, files=files)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    video_id=response.json().get('id')\n",
    "    print(\"Video Uploaded\")\n",
    "    print(f\"Video file ID: {video_id}\")\n",
    "else:\n",
    "    print(f\"Failed to upload file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25111831-4cf8-4560-b246-583204690d87",
   "metadata": {},
   "source": [
    "### 4.2.2 Ingest and Process Video\n",
    "\n",
    "Next, let's process the video to generate dense captions and knowledge graph. This step can take a couple of minutes.\n",
    "- First, we'll set the prompts\n",
    "- Then, we'll call the summarize API to ingest the video\n",
    "- Note that we set ```enable_chat``` to True to create the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2eb82-1733-4346-be59-53a904370853",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"vlm_prompt\": \"You are a warehouse monitoring system. Describe the events in this warehouse and look for any anomalies. \"\n",
    "                            \"Start each sentence with start and end timestamp of the event.\",\n",
    "    \n",
    "    \"caption_summarization\": \"You will be given captions from sequential clips of a video. Aggregate captions in the format \"\n",
    "                             \"start_time:end_time:caption based on whether captions are related to one another or create a continuous scene.\",\n",
    "    \n",
    "    \"summary_aggregation\": \"Based on the available information, generate a summary that captures the important events in the video. \"\n",
    "                           \"The summary should be organized chronologically and in logical sections. This should be a concise, \"\n",
    "                           \"yet descriptive summary of all the important events. The format should be intuitive and easy for a \"\n",
    "                           \"user to read and understand what happened. Format the output in Markdown so it can be displayed nicely.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541af40f-d3f5-4e3b-8ed4-16dace493663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(prompts, video_id=video_id):\n",
    "    \n",
    "    process_video_url = f\"{vss_url}/summarize\"\n",
    "\n",
    "    payload = {\n",
    "        \"id\": video_id,\n",
    "        \"prompt\": prompts['vlm_prompt'],\n",
    "        \"caption_summarization_prompt\": prompts['caption_summarization'],\n",
    "        \"summary_aggregation_prompt\": prompts['summary_aggregation'],\n",
    "        \"model\": \"vila-1.5\",\n",
    "        \"chunk_duration\": 10,\n",
    "        \"chunk_overlap_duration\": 0,\n",
    "        \"summarize\": False,\n",
    "        \"enable_chat\": True,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(process_video_url, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            # Extracting the summary content\n",
    "            summary = response_data.get('choices', [])[0].get('message', {}).get('content', '')\n",
    "            return summary if summary else \"No content received.\"\n",
    "        else:\n",
    "            return f\"Failed to summarize. Status code: {response.status_code}. Response: {response.text}\"\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293beaa8-f9a7-4153-b4ba-27c9322bc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f53b6-2df1-4123-9e7d-4592e90cb84d",
   "metadata": {},
   "source": [
    "### 4.2.3 Ask Questions\n",
    "\n",
    "Once the video is processed, the ```/chat/completions``` endpoint can be called to ask a question\n",
    "\n",
    "![Q&A endpoint](images/qna_swagger.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba17fa-d36b-4021-9c97-76b7fe5a36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qna(query, video_id=video_id):\n",
    "\n",
    "    qna_url = f\"{vss_url}/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"id\": video_id,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": query,\n",
    "                \"role\": \"user\",\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"vila-1.5\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(qna_url, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            # Extracting the answer content\n",
    "            answer = response_data.get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return answer if answer else \"No answer received.\"\n",
    "        else:\n",
    "            return f\"Failed to get a response. Status code: {response.status_code}. Response: {response.text}\"\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bd72b-aced-4a91-8a13-2e31458711c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(\"Was there any forklift in the scene?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe94fe-46fb-4a4d-9e98-86278e719e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(\"Was the worker carrying the box wearing PPE?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e529e6-27f1-43ea-93f2-66e5ef9e10d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.3 Understanding Graph-RAG Components\n",
    "\n",
    "![GraphRAG Diagram](images/GraphRAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29824c1a-336d-4374-a4fc-d136672889f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3.1 G-Extraction/Indexing\n",
    "\n",
    "### Dense Captions to Graph Conversion:\n",
    "The Graph Extractor uses an LLM to analyze dense captions or any text input and identify key entities, actions, and relationships within the text.\n",
    "\n",
    "#### Example:\n",
    "Given a warehouse video scene caption like:  \n",
    "*\"A worker places a heavy box on the conveyor belt, and the box falls due to improper placement.\"*\n",
    "\n",
    "- The LLM can extract entities such as:\n",
    "  - **Worker** (Person)\n",
    "  - **Box** (Object)\n",
    "  - **Conveyor Belt** (Equipment)\n",
    "\n",
    "- Relationships identified might include:\n",
    "  - **\"Worker places box on conveyor belt\"**\n",
    "  - **\"Box falls due to improper placement\"**\n",
    "\n",
    "These entities and relationships are represented as nodes and edges in a Neo4j graph. Captions and embeddings, generated with `nvidia/nv-embedqa-e5-v5`, are also linked to these entities. These can provide descriptive answers to user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0014068-686a-4595-b030-6e5f7d5391ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3.2 G-Retriever\n",
    "\n",
    "### Cypher Query Generation:\n",
    "The Graph Retriever leverages an LLM to process user queries and translate them into structured cypher queries suitable for graph-based searches.\n",
    "\n",
    "#### Example:\n",
    "If the user query is:  \n",
    "*\"What caused the box to fall?\"*\n",
    "\n",
    "- The LLM identifies the key entities (e.g., \"box\") and the desired information (e.g., cause of fall).  \n",
    "- It then generates a structured cypher query for the graph:\n",
    "\n",
    "```cypher\n",
    "MATCH (b:Object)-[:PLACED_ON]->(c:Equipment), (b)-[:FALLS_DUE_TO]->(r:Reason)\n",
    "WHERE b.name = 'Box'\n",
    "RETURN r\n",
    "```\n",
    "\n",
    "This query, executed on the knowledge graph, retrieves the relevant information, enabling users to query complex relationships within the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82000117-356e-4349-9749-8243472b2c21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3.3 G-Generation\n",
    "\n",
    "Once the Graph Retriever processes the user query and fetches a relevant subgraph (entities, relationships, and captions) from the knowledge graph, **G-Generation** utilizes an LLM to analyze and synthesize the retrieved data into a coherent and meaningful response.\n",
    "\n",
    "### Example:\n",
    "If the user query is:  \n",
    "*\"What caused the box to fall?\"*  \n",
    "\n",
    "The Graph Retriever might fetch the subgraph containing:\n",
    "- **Nodes**: \n",
    "  - Object (**Box**)\n",
    "  - Equipment (**Conveyor Belt**)\n",
    "  - Reason (**Improper Placement**)\n",
    "- **Relationships**:\n",
    "  - **\"Box placed on conveyor belt\"**\n",
    "  - **\"Box falls due to improper placement\"**\n",
    "- **Caption**:\n",
    "  - **\"A worker places a heavy box on the conveyor belt, and the box falls due to improper placement.\"**\n",
    "\n",
    "G-Generation processes this data, combining the graph structure and its properties, to generate a response such as:  \n",
    "*\"The box fell because it was improperly placed on the conveyor belt.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d43a8-1fec-47a4-baa4-396b9e1e3a1f",
   "metadata": {},
   "source": [
    "### Let's try a few more questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6c738-3876-470c-8088-32e10614d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(\"What could be some possible safety issues in this warehouse?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5f5b7-9fa0-494e-9780-76f727597667",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(\"When did the worker place the caution tape?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522cd29-fe91-4b68-9f48-1af1487abab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna(\"Describe the warehouse setting in detail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9912f90-374d-4917-bb20-f8092ab520e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qna(\"Enter your question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53154747-e787-4c2d-be15-d7a1278337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qna(\"Enter your question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca0655-f131-439f-9e0b-eb57c6b96bfa",
   "metadata": {},
   "source": [
    "# 4.4 Graph-RAG Visualization\n",
    "\n",
    "In this section, we will explore and visualize the knowledge graph stored in the Neo4j database. By leveraging the Neo4j Python library, we will run queries to fetch specific parts of the graph and render them visually for better understanding. This visualization helps in inspecting the structure and relationships in the graph, providing a clear representation of the data stored in the database.\n",
    "\n",
    "The following cell will generate a link to the Neo4J dashboard. First, copy the web address into `my_url` below.\n",
    "\n",
    "After clicking the generated link, enter the username and password set as part of VSS launch configuration.\n",
    "- For example:\n",
    "  - Username: neo4j\n",
    "  - Password: password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9d668-78bc-4eed-a64a-fd68692faf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "my_url = \"COPY_NOTEBOOK_URL\"\n",
    "my_url = my_url.rsplit(\".com\", 1)[0] + \".com\"\n",
    "neo4j_port = 64018\n",
    "\n",
    "link_html = f'<a href=\"{my_url}:{neo4j_port}\" target=\"_blank\">Click here to open Neo4J Visualizer Dashboard</a>'\n",
    "display(HTML(link_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725cd14-3edd-4712-99db-d2544c0bfdb6",
   "metadata": {},
   "source": [
    "## Sample graph from Neo4j Visualizer Dashboard\n",
    "\n",
    "![Graph Diagram](images/graph_neo4j.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5cc4c-a8ce-4979-be2b-6fa1eef4a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "def visualize_neo4j_query(query, host=\"graph-db\", port=7687, user=\"neo4j\", password=\"password\"):\n",
    "    try:\n",
    "        graph = Graph(f\"bolt://{host}:{port}\", auth=(user, password))\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Neo4j: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        result = graph.run(query)\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        for record in result:\n",
    "            path = record[\"p\"]\n",
    "            for rel in path.relationships:\n",
    "                start_node = rel.start_node\n",
    "                end_node = rel.end_node\n",
    "\n",
    "                start_label = start_node.get(\"name\", start_node.get(\"id\", f\"Node_{start_node.identity}\"))\n",
    "                end_label = end_node.get(\"name\", end_node.get(\"id\", f\"Node_{end_node.identity}\"))\n",
    "\n",
    "                # Wrap labels for better readability if they are too long\n",
    "                start_label = '\\n'.join(textwrap.wrap(start_label, width=20))\n",
    "                end_label = '\\n'.join(textwrap.wrap(end_label, width=20))\n",
    "\n",
    "                G.add_node(start_label)\n",
    "                G.add_node(end_label)\n",
    "                G.add_edge(start_label, end_label, label=rel.__class__.__name__)\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)\n",
    "        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightgreen', node_size=2500)\n",
    "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "        edges = nx.draw_networkx_edges(G, pos, arrowstyle='-|>', arrowsize=10)\n",
    "        edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='blue')\n",
    "\n",
    "        plt.title(\"Neo4j Graph Visualization\")\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running query or visualizing the graph: {e}\")\n",
    "\n",
    "\n",
    "def get_neo4j_query_text(query, host=\"graph-db\", port=7687, user=\"neo4j\", password=\"password\"):\n",
    "    try:\n",
    "        graph = Graph(f\"bolt://{host}:{port}\", auth=(user, password))\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Neo4j: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        result = graph.run(query)\n",
    "        output = []\n",
    "\n",
    "        for record in result:\n",
    "            path = record[\"p\"]\n",
    "            for rel in path.relationships:\n",
    "                start_node = rel.start_node\n",
    "                end_node = rel.end_node\n",
    "\n",
    "                start_label = start_node.get(\"name\", start_node.get(\"id\", f\"Node_{start_node.identity}\"))\n",
    "                end_label = end_node.get(\"name\", end_node.get(\"id\", f\"Node_{end_node.identity}\"))\n",
    "\n",
    "                output.append(f\"Person: {start_label} placed an item: {end_label}\")\n",
    "\n",
    "        if not output:\n",
    "            return \"No results found.\"\n",
    "\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running query or processing the results: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ccc0b-da3c-4556-ba3a-e2577b55e2d9",
   "metadata": {},
   "source": [
    "## Part 4.4.1 Cypher queries\n",
    "\n",
    "<span style=\"color:red\"><b>NOTE: You might have to modify the entity and relationship names in the following cypher queries based on the actual generated graph</b></span>\n",
    "\n",
    "#### Visualizing Who Wears What\n",
    "\n",
    "Let's see how the sub-graph related to all entities with keywork \"WEARS\" looks like:\n",
    "\n",
    "The following Cypher query retrieves and visualizes relationships where people are wearing items. It matches all `WEARS` relationships in the graph and returns the paths to better understand the connections.\n",
    "\n",
    "**Cypher Query:**\n",
    "```cypher\n",
    "MATCH p=()-[r:WEARS]->() \n",
    "RETURN p\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c884591-0909-4f27-8c14-5c30ffe26867",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_neo4j_query(\"MATCH p=()-[r:WEARS]->() RETURN p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d2e5a-0ce3-430f-84df-bfaa559a0b83",
   "metadata": {},
   "source": [
    "#### Visualizing Sub-Graph where a Person with id='worker' carries an item\n",
    "\n",
    "The following Cypher query retrieves information about a specific person (identified by `worker`) who has carries an item. It matches the `CARRIES` relationship between the person and the item, returning the path and details of the action.\n",
    "\n",
    "**Cypher Query:**\n",
    "\n",
    "```cypher\n",
    "MATCH p=(person)-[r:CARRIES]->(item)\n",
    "WHERE person.id = 'worker'\n",
    "RETURN p\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f197ef-4a19-40ff-9ce3-1d5111fd5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_neo4j_query(\"MATCH p=(person)-[r:CARRIES]->(item) WHERE person.id = 'worker' RETURN p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7631d8-c903-4637-81d3-5dbb075063f2",
   "metadata": {},
   "source": [
    "#### Fetching a particular node with Cypher Query\n",
    "\n",
    "Let's see how our system would answer \"Describe the person who was carrying the box.\" which we asked before.\n",
    "\n",
    "The following Cypher query retrieves information about people who carried an item with the `id` of \"box\". It matches the `CARRIES` relationship between a person and the item, and returns the details of the person and the item they carried.\n",
    "\n",
    "**Cypher Query:**\n",
    "\n",
    "```cypher\n",
    "MATCH p=(person)-[r:CARRIES]->(item)\n",
    "WHERE item.id = \"box\"\n",
    "RETURN p\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa7d5c-278f-44f5-b52d-4ff72aa2fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH p=(person)-[r:CARRIES]->(item)\n",
    "WHERE item.id = \"box\"\n",
    "RETURN p\n",
    "\"\"\"\n",
    "\n",
    "text_output = get_neo4j_query_text(query)\n",
    "print(text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d01a6-087a-40c6-9708-1c1dee0776af",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e81c8-bf0b-41d1-b5cc-8e106693a4ee",
   "metadata": {},
   "source": [
    "Congratulations! This concludes the final lab of the course. Time to put this knowledge to the test in a final assessment. Best of luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64180019-fa9b-426e-9da1-8b27b161a645",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
