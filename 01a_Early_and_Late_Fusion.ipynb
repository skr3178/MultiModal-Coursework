{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-a8GRH8DXKk"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTQNPPpeDaOk"
   },
   "source": [
    "# 1a. Early and Late Fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81PfOMFwKBZG"
   },
   "source": [
    "Multimodal models are a simple concept with a surprisingly complex practice. Let's compare different data types and how we can analyze them in a multimodal model with a robotics use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pkc4vj3csv3"
   },
   "source": [
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Explore the properties of LiDAR data\n",
    "* Compare single modal models\n",
    "  * Construct an RGB image model\n",
    "  * Construct a LiDAR model\n",
    "* Compare multimodal models\n",
    "  * Construct a late fusion model\n",
    "  * Construct and early fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 6260,
     "status": "error",
     "timestamp": 1689940390619,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "NNmWEhrB-uSm",
    "outputId": "6450de47-6418-4268-82b0-da2f8e3b9fe0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPy_img\n",
    "\n",
    "import utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAAgjE5K_ZFZ"
   },
   "source": [
    "In PyTorch, we can use our GPU in our operations by setting the [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) to `cuda`. The function `torch.cuda.is_available()` will confirm PyTorch can recognize the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "Dw5___FcYtBQ",
    "outputId": "f418f7d6-cf0c-4b23-87ad-140cc2b1501c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1689675950620,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "dfE6Lkmq_Tt4",
    "outputId": "b346ac80-72b6-4db0-e76b-f8cff013d312"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzHZsOWLCdmB"
   },
   "source": [
    "## 1.1 The Dataset\n",
    "\n",
    "Many fields of data science have famous \"Hello World\" style datasets that can be used to demonstrate a model architecture. Unfortunately, there is no such standard benchmark for multimodal models, so we decided to make such a dataset using [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/).\n",
    "\n",
    "Real world data collection can be challenging, and so NVIDIA Omniverse is a platform that can be used for creating and simulating virtual copies of real-life objects and locations. These virtual copies are called [Digital Twins](https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/). These digital twins can be used to simulate and capture data that would be difficult, or even impossible, with real world instruments. The process of creating this data is called [Synthetic Data Generation](https://www.nvidia.com/en-us/use-cases/synthetic-data/).\n",
    "\n",
    "For this first lab, we randomly generated the positions of three objects: a cube, a sphere, or a torus (a.k.a. a donut). What kind of tools could we use to determine the position of the center of these objects? Let's look at our options.\n",
    "\n",
    "Each of the data types below are generated from the same position of objects. The position is the same across data files of the same file number, in this case, `0163`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 RGB Camera Data\n",
    "The first kind of data are [RGB color images](https://en.wikipedia.org/wiki/RGB_color_model). This could be captured using a camera, such as the one on the back of most smartphones. Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIRaYbZkDKQa"
   },
   "outputs": [],
   "source": [
    "data_img = Image.open('data/replicator_data_parallel/rgb/0163.png')\n",
    "plt.imshow(data_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have an image that's 64 pixels tall and 64 pixels wide. Image data like this is generally analyzed using a [convolutional neural network](https://developer.nvidia.com/discover/convolutional-neural-network). We will be using the same technique in our multimodal models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Distance Image\n",
    "\n",
    "Since the data is simulated, Omniverse can calculate the distance of each pixel to the camera that is capturing the image. RGB cameras work by recording the light that reflects off of the objects in front of it. Because of that, it can be difficult to gather usable data in low-light situations. By instead using the distance, we can see better in the dark.\n",
    "\n",
    "However, in the real world, there are not many instruments that can capture this data like in the image below. Because of this, we will not be using them in our multimodal models, but the data is available to help verify our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1689675953471,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "0ICXTYpQDyjv",
    "outputId": "bcb0235c-6d89-473d-997a-128ebc1ef0bb"
   },
   "outputs": [],
   "source": [
    "data = np.load('data/replicator_data_parallel/distance/0163.npy')\n",
    "plt.imshow(data, cmap=utils.cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 LiDAR Data\n",
    "\n",
    "One real world instrument that can capture distance data is a [LiDAR](https://oceanservice.noaa.gov/facts/lidar.html) sensor. LiDAR sensors work by emitting multiple beams of light invisible to the human eye. Thanks to [Ole RÃ¸mer](https://en.wikipedia.org/wiki/Ole_R%C3%B8mer) who calculated the speed of light, we can measure how far away something is based on how long it takes to observe the emitted light.\n",
    "\n",
    "There are many types of LiDAR devices available on the market, but to make a better comparison with RGB data, we simulated 64 by 64 lasers being emitted from the sensor. Because of the shape, we can view it like an image. There will be a little bit of a discrepancy because LiDAR data is collected along a curve, but this 2D projection can still be useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIU9PzSJEdTp"
   },
   "outputs": [],
   "source": [
    "data = np.load('data/replicator_data_parallel/lidar/0163.npy')\n",
    "plt.imshow(data, cmap=utils.cmap)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, the objects look familiar, but they appear rotated compared to our RGB and Distance images. That's because each light beam is emitted at a different angle, so to align the LiDAR data with the RGB information, we need the angle information. If we have the angles, not only would we have the distance traveled of each emitted laser beam, we could also calculate the position of where the laser landed. \n",
    "\n",
    "Because this is a 3-dimensional space, we need at least two angles to calculate the position. Thankfully, we have this data. The [azimuth](https://en.wikipedia.org/wiki/Azimuth) represents the horizontal rotation from the LiDAR sensor's forward, and the [zenith](https://en.wikipedia.org/wiki/Zenith) represents the vertical rotation.\n",
    "\n",
    "<center><img src=\"images/lidar_math.png\" width=\"70%\"/></center>\n",
    "\n",
    "This particular LiDAR system has the following properties to consider:\n",
    "* The azimuth angle corresponds to the rows, and the zenith angle corresponds to the columns.\n",
    "* The angles use a [left-handed coordinate system](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/geometry/coordinate-systems.html#:~:text=The%20differentiation%20between%20left%2Dhanded,a%20right%2Dhand%20coordinate%20system.) with Z as the vertical axis.\n",
    "* Azimuth and zenith as measured relative to a point of reference.\n",
    "\n",
    "This is not true of all LiDAR systems, so the following calculations may be different if a different system is used. Let's break down the steps as much of the logic can be applied to most azimuth/zenith systems.\n",
    "\n",
    "First, we'll need the azimuth of each laser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuth = np.load(\"data/replicator_data_parallel/azimuth.npy\")\n",
    "print(azimuth.shape)\n",
    "azimuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll need the zenith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zenith = np.load(\"data/replicator_data_parallel/zenith.npy\")\n",
    "print(zenith.shape)\n",
    "zenith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy trick to remember whether to use sine or cosine. sin(0) = 0 and cos(0) is 1. We're using an XYZ coordinate plane where X is the horizontal position, Y is the forward and backward position, and Z is the vertical position. If we fire our laser straight out in front of us, the position where it lands should be (0,d,0) where d is the distance the laser travelled.\n",
    "\n",
    "Based on how this particular LiDAR system collected its data, the sine of the azimuth will help us calculate our X positions and make sure x is 0 when the azimuth is 0. We will add a dimension with `None` so we can [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) the data into the same shape as the lidar depth data.\n",
    "\n",
    "Notice the `-` sign by `azimuth`? Come back later and remove it to see how it affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_surface = np.ones_like(data) * np.sin(-azimuth[:, None])\n",
    "plt.imshow(x_surface, cmap=utils.cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! This is why our LiDAR data looks rotated. For this data, the azimuth changes from top to bottom, not from left to right. We can see a similar phenomenon when we calculate the zenith. Using the same logic as above. we will use the sine of the zenith to make sure z is 0 when the zenith is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_surface = np.ones_like(data) * np.sin(-zenith[None, :])\n",
    "plt.imshow(z_surface, cmap=utils.cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've calculated our Xs and our Zs, so finally, it's the Ys. When we fire a laser directly in front of us, the y position should be the same as the distance the laser traveled. However, the more the angle of the laser deviates, the more the y position is less than the distance travelled. To calculate this, we will use the cosine of both the azimuth and the zenith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_surface = np.ones_like(data) * np.cos(-azimuth[:, None]) * np.cos(-zenith[None, :])\n",
    "plt.imshow(y_surface, cmap=utils.cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more thing to consider about LiDAR data. Many LiDAR sensors have a max range where they are effective. For this sensor system, if it doesn't detect that the light has returned, it will assume the maximum distance for that laser emitted. To make better visualizations, let's create a mask to identify when a laser has travelled its maximum distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [data != data.max()][0]\n",
    "plt.imshow(a, cmap=utils.cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've calculate how much the azimuth and the zenith alter the distance travelled, all we have to do is multiple our X, Y and Z surfaces with the lidar depth to extract the position information of each laser collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_depth = data\n",
    "\n",
    "x = lidar_depth * x_surface\n",
    "z = lidar_depth * z_surface\n",
    "y = lidar_depth * y_surface\n",
    "\n",
    "plt.clf()\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(x, cmap=utils.cmap)\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(z, cmap=utils.cmap)\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(y, cmap=utils.cmap)\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(a, cmap=utils.cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many LiDAR visualizations, points are colored based on their vertical position. Let's do the same. We'll use `c` to represent how to color a collision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.copy(z)\n",
    "c_min = np.min(c)\n",
    "c_max = np.max(c)\n",
    "c = (c - c_min) / (c_max - c_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our position information, let's plot it. How does it look? Is the LiDAR data finally aligned with the RGB image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot the points\n",
    "x_scatter = x[a == 1]\n",
    "y_scatter = y[a == 1]\n",
    "z_scatter = z[a == 1]\n",
    "c_scatter = c[a == 1]\n",
    "ax.scatter(x_scatter, y_scatter, z_scatter, c=c_scatter, cmap=\"rainbow\", marker='o')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(19, 31)\n",
    "ax.set_zlim(-6, 6)\n",
    "\n",
    "ax.view_init(elev=0., azim=270)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/replicator_data_parallel/rgb/0163.png\" width=\"300px\" style=\"image-rendering:pixelated;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Since this is position information, we can take our visualization further. Let's create an animation that rotates the points in a looping GIF.\n",
    "\n",
    "This may take a few minutes. It's worth it, we promise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    ax.scatter(x_scatter, y_scatter, z_scatter, c=c_scatter, cmap=\"rainbow\", marker='o')\n",
    "    return fig,\n",
    "\n",
    "def animate(i):\n",
    "    ax.view_init(elev=30., azim=i)\n",
    "    return fig,\n",
    "\n",
    "file_path = 'pointcloud.gif'\n",
    "if not os.path.exists(file_path):\n",
    "    anim = utils.animation.FuncAnimation(\n",
    "        fig, animate, init_func=init, frames=360, interval=20, blit=True\n",
    "    )\n",
    "    anim.save(file_path, fps=30)\n",
    "IPy_img(open(file_path,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb_BfRbWEPyR"
   },
   "source": [
    "## 1.2 Model Comparison\n",
    "\n",
    "Now that we've explored our data, it's time to create our models. Before making a multimodal model, let's see how a single modal model would fair. We'll make a model that only uses RGB images and a model that only uses LiDAR data.\n",
    "\n",
    "### 1.2.1 PyTorch Data\n",
    "\n",
    "Before we can create our models, we should convert our data into a [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). First, let's create a function that will calculate the XYZ position data from the lidar depth data. We will use the letter `a` to represent whether the depth is at maximum or not.\n",
    "\n",
    "**TODO:** Create a function to convert lidar depth data into position data by replacing the `FIXME`s below. *Hint*: The PyTorch implementation is similar to the NumPy implementation. Click the `...` for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_xyza(lidar_depth, azimuth, zenith):\n",
    "    x = lidar_depth * torch.sin(FIXME)\n",
    "    z = lidar_depth * torch.sin(FIXME)\n",
    "    y = lidar_depth * torch.cos(FIXME) * torch.cos(FIXME)\n",
    "    a = torch.where(lidar_depth < 50.0, torch.ones_like(lidar_depth), torch.zeros_like(lidar_depth))\n",
    "    xyza = torch.stack((x, y, z, a))\n",
    "    return xyza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_torch_xyza(lidar_depth, azimuth, zenith):\n",
    "    x = lidar_depth * torch.sin(-azimuth[:, None])\n",
    "    z = lidar_depth * torch.sin(-zenith[None, :])\n",
    "    y = lidar_depth * torch.cos(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n",
    "    a = torch.where(lidar_depth < 50.0, torch.ones_like(lidar_depth), torch.zeros_like(lidar_depth))\n",
    "    xyza = torch.stack((x, y, z, a))\n",
    "    return xyza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our XYZ data in tensor format, let's create a PyTorch Dataset. Normally, this is where we would apply data augmentation. However, we will be using this dataset for our multimodal model as well. Any data augmentation such as cropping and flipping would need to apply to both the corresponding image and LiDAR data. The math for that would get a little complicated, so let's keep things simple for now and use the data as is for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEV5ZSK0Hj6l"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.root_dir = root_dir\n",
    "        self.imgs = []\n",
    "        self.lidar_depths = []\n",
    "        self.positions = np.genfromtxt(\n",
    "            root_dir + \"positions.csv\", delimiter=\",\", skip_header=1\n",
    "        )[start_idx:stop_idx]\n",
    "\n",
    "        self.azimuth = torch.from_numpy(azimuth).to(device)\n",
    "        self.zenith = torch.from_numpy(zenith).to(device)\n",
    "\n",
    "        for idx in range(start_idx, stop_idx):\n",
    "            file_number = \"{:04d}\".format(idx)\n",
    "            rbg_img = Image.open(self.root_dir + \"rgb/\" + file_number + \".png\")\n",
    "            rbg_img = img_transforms(rbg_img).to(device)\n",
    "            self.imgs.append(rbg_img)\n",
    "\n",
    "            lidar_depth = np.load(self.root_dir + \"lidar/\" + file_number + \".npy\")\n",
    "            lidar_depth = torch.from_numpy(lidar_depth).to(torch.float32).to(device)\n",
    "            self.lidar_depths.append(lidar_depth)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.imgs[idx]\n",
    "        lidar_depth = self.lidar_depths[idx]\n",
    "        lidar_xyza = get_torch_xyza(lidar_depth, self.azimuth, self.zenith)\n",
    "\n",
    "        position = self.positions[idx]\n",
    "        position = torch.from_numpy(position).to(torch.float32).to(device)\n",
    "\n",
    "        return rbg_img, lidar_xyza, position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we feed this into our neural networks for training, let's test it out. Is the data in the shape and size we would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(\"data/replicator_data_parallel/\", 0, N-VALID_BATCHES*BATCH_SIZE)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"data/replicator_data_parallel/\", N-VALID_BATCHES*BATCH_SIZE, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order is `rbg_img`, `lidar_xyza`, and then `position`. The `rbg_img` data and `lidar_xyza` data should be the same shapes as we are comparing them. We have 3 objects, and we're trying to find the x, y, and z positions of the centers of each of them. Therefore, the position information should have 9 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(train_data):\n",
    "    print(i, sample[0].shape, sample[1].shape, sample[2].shape)\n",
    "\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Single Modal Model Architecture\n",
    "\n",
    "Before we make a multimodal model, let's see how a single modal model will perform. There are many ways we could create this model, so it would be difficult to prove one data type is better than the other in all situations. That said, it's still worthwhile to hypothesize why some data might be better than others. In addition, we can use our single modal models as a baseline for our multimodal models.\n",
    "\n",
    "Given the shape for both the image and LiDAR data is the same, we can use a convolutional neural network for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positions = 9\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "        self.conv1 = nn.Conv2d(in_ch, 50, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        self.conv3 = nn.Conv2d(100, 200, kernel_size, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(200 * 8 * 8, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, num_positions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of loss function would be the best? Given that we're trying to predict position information, why not use the [Pythagorean Theorem](https://en.wikipedia.org/wiki/Pythagorean_theorem) to calculate the distance between our predicted positions and the actual positions. Then we can try to minimize that distance through gradient descent.\n",
    "\n",
    "Good news! The [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) loss function will do that for us. The mean squared error is the same as the Pythagorean Theorem, but without the square root. Since minimizing the MSE would also minimize the RMSE, we can use the MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our training loop. We've moved some of the functions into [utils.py](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) for brevity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, inputs_idx, epochs=20):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs, target = utils.get_outputs(model, batch, inputs_idx)\n",
    "            loss = loss_func(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / (step + 1)\n",
    "        train_losses.append(train_loss)\n",
    "        utils.print_loss(epoch, train_loss, outputs, target, is_train=True)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        for step, batch in enumerate(valid_dataloader):\n",
    "            outputs, target = utils.get_outputs(model, batch, inputs_idx)\n",
    "            valid_loss += loss_func(outputs, target).item()\n",
    "        valid_loss = valid_loss / (step + 1)\n",
    "        valid_losses.append(valid_loss)\n",
    "        utils.print_loss(epoch, valid_loss, outputs, target, is_train=False)\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to get this experiment started! This may take a little while, so please press the fast forward button ![image](images/fast_forward.png) above to run the rest of the notebook. While the experiment is running, let's take a moment to think. Which data type do you think would be more effective at predicting the location of our objects? Can you give a reason why?\n",
    "\n",
    "Once you have an answer, please continue reading the rest of the notebook, even if the next cell is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "rgb_net = Net(4).to(device)\n",
    "rgb_opt = Adam(rgb_net.parameters(), lr=0.0001)\n",
    "\n",
    "xyz_net = Net(4).to(device)\n",
    "xyz_opt = Adam(xyz_net.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"Training rgb_net\")\n",
    "rgb_train_loss, rgb_valid_loss = train_model(rgb_net, rgb_opt, 0)\n",
    "print(\"Training xyz_net\")\n",
    "xyz_train_loss, xyz_valid_loss = train_model(xyz_net, xyz_opt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_x = range(epochs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(plot_x, rgb_train_loss, \"green\", label = \"RGB Train Loss\")\n",
    "plt.plot(plot_x, rgb_valid_loss, \"darkgreen\", label = \"RGB Valid Loss\")\n",
    "plt.plot(plot_x, xyz_train_loss, \"orchid\", label = \"XYZ Train Loss\")\n",
    "plt.plot(plot_x, xyz_valid_loss, \"darkorchid\", label = \"XYZ Valid Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Multimodal Model Architecture\n",
    "\n",
    "If you've gotten to this point and the experiment is still running, we won't spoil the surprise. In the meanwhile, let's get another experiment up and running. Once we have pretrained models for each the image data and the LiDAR data, there's a relatively simple way we can turn them into a multimodal modal: by connecting the outputs of each model into a final output. In this way, it's almost like we're creating an [ensemble model](https://scikit-learn.org/1.5/modules/ensemble.html), where each model has a weighted vote in the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [rgb_net, xyz_net]\n",
    "\n",
    "for network in networks:\n",
    "    for param in network.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "class MyMultimodalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rgb_net = rgb_net\n",
    "        self.xyz_net = xyz_net\n",
    "        self.fc1 = nn.Linear(num_positions * len(networks), num_positions * 10)\n",
    "        self.fc2 = nn.Linear(num_positions * 10, num_positions)\n",
    "\n",
    "    def forward(self, x_img, x_xyz):\n",
    "        x_rgb = self.rgb_net(x_img)\n",
    "        x_xyz = self.xyz_net(x_xyz)\n",
    "        x = torch.cat((x_rgb, x_xyz), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of multimodal model is called `late fusion` because we analyze each data type separately and only combine the two data streams at the very end. In our [utils.py](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) file, we've created a flexible function similar to the `train_model` above in order to train this model. It still needs a function to extract the network inputs from the dataloader, so let's define that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_late_inputs(batch):\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    return (inputs_rgb, inputs_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's run another experiment. Will this be better or worse than the single modal models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_late_net = MyMultimodalModel().to(device)\n",
    "mm_late_opt = Adam(mm_late_net.parameters(), lr=0.0001)\n",
    "mm_late_train_loss, mm_late_valid_loss = utils.train_model(\n",
    "    mm_late_net,\n",
    "    mm_late_opt,\n",
    "    get_mm_late_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got one more experiment to run in this lab. We have late fusion, so why not try early fusion? If late fusion combines the data pathways at the end of the model, early fusion does it right at the beginning. In fact, we will use the same model architecture as our `rbg_net` and `xyz_net`. Here, we'll stack the two dataypes on top of each other and treat it like one data type with twice the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mm_early_inputs(batch):\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    inputs_mm_early = torch.cat((inputs_rgb, inputs_xyz), 1)\n",
    "    return (inputs_mm_early,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do you think? Between early fusion and late fusion, which one is more effective in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_early_net = Net(8).to(device)\n",
    "mm_early_opt = Adam(mm_early_net.parameters(), lr=0.0001)\n",
    "mm_early_train_loss, mm_early_valid_loss = utils.train_model(\n",
    "    mm_early_net,\n",
    "    mm_early_opt,\n",
    "    get_mm_early_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(plot_x, xyz_train_loss, \"goldenrod\", label = \"LiDAR Train Loss\")\n",
    "plt.plot(plot_x, xyz_valid_loss, \"darkgoldenrod\", label = \"LiDAR Valid Loss\")\n",
    "plt.plot(plot_x, mm_late_train_loss, \"green\", label = \"Late Fusion Train Loss\")\n",
    "plt.plot(plot_x, mm_late_valid_loss, \"darkgreen\", label = \"Late Fusion Valid Loss\")\n",
    "plt.plot(plot_x, mm_early_train_loss, \"orchid\", label = \"Early Fusion Train Loss\")\n",
    "plt.plot(plot_x, mm_early_valid_loss, \"darkorchid\", label = \"Early Fusion Valid Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've gotten this far and are still waiting for the experiment to complete, we've create a bonus notebook to explore more interesting data types while we're waiting for training to finish.\n",
    "\n",
    "If the experiment is over, what were the results? Were they what you expected? Let's explore the theory in the next slide deck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
