{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73275a49",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60ab08",
   "metadata": {},
   "source": [
    "# 5. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fdc5d",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hope it was a fun journey with some new skills as souvenirs. Now it's time to put those skills to the test.\n",
    "\n",
    "Here's the challenge: Let's say we have a have a classification model that uses LiDAR data to classify spheres and cubes. Compared to RGB cameras, LiDAR sensors are not as easy to come by, so we'd like to convert this model so it can classify RGB images instead. Since we used [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) to generate LiDAR and RGB data pairs, let's use this data to create a contrastive pre-training model. Since CLIP is already taken, we will call this model `CILP` for \"Contrastive Image LiDAR Pre-training\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1083571",
   "metadata": {},
   "source": [
    "## 5.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb74e7",
   "metadata": {},
   "source": [
    "Let's get started. Below are the libraries used in this assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c241ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from assessment import assesment_utils\n",
    "from assessment.assesment_utils import Classifier\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30f447-41fd-4b60-9e11-d325ca379bb7",
   "metadata": {},
   "source": [
    "### 5.1.1 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69e9d3",
   "metadata": {},
   "source": [
    "Next, let's load our classification model and call it `lidar_cnn`. If we take a moment to view the [assement_utils](assessment/assesment_utils.py), we can see the `Classifier` class used to construct the model. Please note the `get_embs` method, which we will be using to construct our cross-modal projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cf3584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedder): Sequential(\n",
       "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_cnn = Classifier(1).to(device)\n",
    "lidar_cnn.load_state_dict(torch.load(\"assessment/lidar_cnn.pt\", weights_only=True))\n",
    "# Do not unfreeze. Otherwise, it would be difficult to pass the assessment.\n",
    "for param in lidar_cnn.parameters():\n",
    "    lidar_cnn.requires_grad = False\n",
    "lidar_cnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b5415-e9ef-442e-a2fa-f4d58747cd62",
   "metadata": {},
   "source": [
    "### 5.1.2 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313fe37",
   "metadata": {},
   "source": [
    "Below is the dataset we will be using in this assessment. It is similar to the dataset we used in the first few labs, but please note `self.classes`. Unlike the first lab where we predicted position, in this lab, we will determine whether the RGB or LiDAR we are evaluating contains a `cube` or a `sphere`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Scales data into [0,1]\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "    \n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d446-2f2c-4c44-a254-a3f34492685f",
   "metadata": {},
   "source": [
    "This data is available in the `/data/assessment` folder. Here is an example of one of the cubes. The images are small, but there is enough detail that our models will be able to tell the difference.\n",
    "\n",
    "<center><img src=\"data/assessment/cubes/rgb/0002.png\" /></center>\n",
    "\n",
    "Let's go ahead and load the data into a `DataLoader`. We'll set aside a few batches (`VALID_BATCHES`) for validation. The rest of the data will be used for training. We have `9999` images for each of the cube and sphere categories, so we'll multiply N times 2 to reflect the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49e3ec1-7598-44bf-8872-b130eb90b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VALID_BATCHES = 10\n",
    "N = 9999\n",
    "\n",
    "valid_N = VALID_BATCHES*BATCH_SIZE\n",
    "train_N = N - valid_N\n",
    "\n",
    "train_data = MyDataset(\"data/assessment/\", 0, train_N)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_data = MyDataset(\"data/assessment/\", train_N, N)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "N *= 2\n",
    "valid_N *= 2\n",
    "train_N *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d3353",
   "metadata": {},
   "source": [
    "## 5.2 Contrastive Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541da32",
   "metadata": {},
   "source": [
    "Before we create a cross-modal projection model, it would be nice to have a way to embed our RGB images as a starting point. Let's be efficient with our data and create a contrastive pre-training model. First, it would help to have a convolutional model. We've prepared a recommended architecture below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba7c61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, in_ch, emb_size=CILP_EMB_SIZE):\n",
    "        super().__init__()\n",
    "        kernel_size = 3\n",
    "        stride = 1\n",
    "        padding = 1\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 50, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(50, 100, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(100, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(200, 200, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.dense_emb = nn.Sequential(\n",
    "            nn.Linear(200 * 4 * 4, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        emb = self.dense_emb(conv)\n",
    "        return F.normalize(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ad7f",
   "metadata": {},
   "source": [
    "The RGB data has `4` channels, and our LiDAR data has `1`. Let's initiate these embedding models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4b81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embedder = Embedder(4).to(device)\n",
    "lidar_embedder = Embedder(1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4247201",
   "metadata": {},
   "source": [
    "Now that we have our embedding models, let's combine them into a `ContrastivePretraining` model.\n",
    "\n",
    "**TODO**: The `ContrastivePretraining` class below is almost done, but it has a few `FIXME`s. Please replace the FIXMEs to have a working model. Feel free to review notebook [02b_Contrastive_Pretraining.ipynb](02b_Contrastive_Pretraining.ipynb) for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c09a3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePretraining(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_embedder = img_embedder\n",
    "        self.lidar_embedder = lidar_embedder\n",
    "        self.cos = nn.CosineSimilarity()\n",
    "\n",
    "    def forward(self, rgb_imgs, lidar_depths):\n",
    "        img_emb = self.img_embedder(rgb_imgs)\n",
    "        lidar_emb = self.lidar_embedder(lidar_depths)\n",
    "\n",
    "        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)\n",
    "        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)\n",
    "\n",
    "        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n",
    "        similarity = torch.unflatten(similarity, 0, (BATCH_SIZE, BATCH_SIZE))\n",
    "        similarity = (similarity + 1) / 2\n",
    "\n",
    "        logits_per_img = similarity\n",
    "        logits_per_lidar = similarity.T\n",
    "        return logits_per_img, logits_per_lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83430633",
   "metadata": {},
   "source": [
    "Before we can train the model, we should define a loss function to guide our model in learning.\n",
    "\n",
    "**TODO**: The `get_CILP_loss` function below is almost done. Do you remember the formula to calculate the loss? Please replace the `FIXME`s below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "181b9036-a22f-474d-b629-7a210dcbfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CILP_loss(batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    logits_per_img, logits_per_lidar = CILP_model(rbg_img, lidar_depth)\n",
    "    total_loss = (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth))/2\n",
    "    return total_loss, logits_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62782",
   "metadata": {},
   "source": [
    "Time to put these models to the test! First, let's initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7a4a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_model = ContrastivePretraining().to(device)\n",
    "optimizer = Adam(CILP_model.parameters(), lr=0.0001)\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182b887-ef15-4cf1-9916-161f8db05a24",
   "metadata": {},
   "source": [
    "Next, it's time to train. If the above `TODO`s were completed correctly, the loss should be under `3.2`. Are the values along the diagional close to `1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4db7204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train Loss: 3.0879874265016016 \n",
      "Similarity:\n",
      "tensor([[0.9972, 0.0379, 0.4454,  ..., 0.2381, 0.5527, 0.2554],\n",
      "        [0.0501, 0.9845, 0.4564,  ..., 0.8475, 0.4028, 0.7434],\n",
      "        [0.3975, 0.5722, 0.9886,  ..., 0.1722, 0.7463, 0.4071],\n",
      "        ...,\n",
      "        [0.2195, 0.8055, 0.1573,  ..., 0.9925, 0.2809, 0.8099],\n",
      "        [0.6167, 0.3427, 0.7876,  ..., 0.2268, 0.9913, 0.6406],\n",
      "        [0.2538, 0.6974, 0.3776,  ..., 0.8134, 0.6218, 0.9943]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.191289500186318 \n",
      "Similarity:\n",
      "tensor([[0.9929, 0.8805, 0.4261,  ..., 0.4011, 0.4511, 0.9885],\n",
      "        [0.8579, 0.9958, 0.2482,  ..., 0.1951, 0.2363, 0.9011],\n",
      "        [0.3929, 0.2471, 0.9942,  ..., 0.5831, 0.5269, 0.3459],\n",
      "        ...,\n",
      "        [0.4149, 0.2071, 0.5989,  ..., 0.9976, 0.9893, 0.3666],\n",
      "        [0.4750, 0.2561, 0.5403,  ..., 0.9925, 0.9973, 0.4286],\n",
      "        [0.9905, 0.9180, 0.3655,  ..., 0.3684, 0.4208, 0.9969]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1\n",
      "Train Loss: 3.0303730276686633 \n",
      "Similarity:\n",
      "tensor([[0.9973, 0.8780, 0.3460,  ..., 0.5600, 0.3758, 0.6526],\n",
      "        [0.7641, 0.9680, 0.5874,  ..., 0.4422, 0.4469, 0.4545],\n",
      "        [0.3476, 0.4867, 0.9973,  ..., 0.2549, 0.5004, 0.2614],\n",
      "        ...,\n",
      "        [0.6099, 0.5426, 0.2970,  ..., 0.9925, 0.2776, 0.2897],\n",
      "        [0.3745, 0.4205, 0.4676,  ..., 0.2855, 0.9949, 0.8599],\n",
      "        [0.6306, 0.5417, 0.2511,  ..., 0.3145, 0.8437, 0.9979]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1809636919121993 \n",
      "Similarity:\n",
      "tensor([[0.9981, 0.7631, 0.3323,  ..., 0.4675, 0.5711, 0.9775],\n",
      "        [0.7584, 0.9974, 0.3826,  ..., 0.2378, 0.2732, 0.8273],\n",
      "        [0.3580, 0.3813, 0.9944,  ..., 0.5565, 0.4476, 0.2853],\n",
      "        ...,\n",
      "        [0.4936, 0.2309, 0.5367,  ..., 0.9979, 0.9834, 0.4588],\n",
      "        [0.5981, 0.2632, 0.4112,  ..., 0.9686, 0.9972, 0.5643],\n",
      "        [0.9781, 0.8198, 0.2610,  ..., 0.4403, 0.5450, 0.9988]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2\n",
      "Train Loss: 3.0225284404027124 \n",
      "Similarity:\n",
      "tensor([[0.9969, 0.5134, 0.3209,  ..., 0.4089, 0.8150, 0.4239],\n",
      "        [0.4546, 0.9970, 0.3780,  ..., 0.4373, 0.4483, 0.3713],\n",
      "        [0.3147, 0.3971, 0.9948,  ..., 0.4920, 0.3156, 0.5677],\n",
      "        ...,\n",
      "        [0.3918, 0.4490, 0.5083,  ..., 0.9990, 0.1709, 0.9834],\n",
      "        [0.7712, 0.4730, 0.3398,  ..., 0.1659, 0.9954, 0.2077],\n",
      "        [0.4217, 0.3978, 0.5931,  ..., 0.9817, 0.2128, 0.9985]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Valid Loss: 3.1770610432875785 \n",
      "Similarity:\n",
      "tensor([[0.9962, 0.6303, 0.3416,  ..., 0.4896, 0.6001, 0.9490],\n",
      "        [0.6426, 0.9988, 0.4059,  ..., 0.2671, 0.2538, 0.7560],\n",
      "        [0.3387, 0.3811, 0.9951,  ..., 0.5967, 0.4661, 0.2615],\n",
      "        ...,\n",
      "        [0.4700, 0.2586, 0.6063,  ..., 0.9991, 0.9714, 0.4353],\n",
      "        [0.5881, 0.2524, 0.4746,  ..., 0.9711, 0.9991, 0.5445],\n",
      "        [0.9654, 0.7516, 0.2586,  ..., 0.4517, 0.5564, 0.9985]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    CILP_model.train()\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    assesment_utils.print_CILP_results(epoch, train_loss/step, logits_per_img, is_train=True)\n",
    "\n",
    "    CILP_model.eval()\n",
    "    valid_loss = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        loss, logits_per_img = get_CILP_loss(batch)\n",
    "        valid_loss += loss.item()\n",
    "    assesment_utils.print_CILP_results(epoch, valid_loss/step, logits_per_img, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb8027-4ffb-4ff8-81a2-395f5c76ffcb",
   "metadata": {},
   "source": [
    "When complete, please freeze the model. We will assess this model with our cross-model projection model, and if this model is altered during cross-model projection training, it may not pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea35a356-4536-494b-98c9-806dced9ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf714",
   "metadata": {},
   "source": [
    "## 5.3 Cross-Modal Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674845f5",
   "metadata": {},
   "source": [
    "Now that we have a way to embed our image data, let's move on to cross-modal projection. \n",
    "\n",
    "**TODO**: Let's jump right in and create the projector. What should be the dimensions into the model, and what should be the dimensions out of the model? A hint to the first `FIXME` can be found in section [#5.2-Contrastive-Pre-training](#5.2-Contrastive-Pre-training) in the `Embedding` class. A hint to the second `FIXME` can be found in the [assessment/assesment_utils.py](assessment/assesment_utils.py) file in the `Classifier` class. The dimensions of the second `FIXME` should be the same size as the output of the `get_embs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ba3d769-afc5-4749-a271-1d59a0a830bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CILP_EMB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6427d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(CILP_EMB_SIZE, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 1)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "#FIXME : clip_emb_size[0] and vgg_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457189",
   "metadata": {},
   "source": [
    "Next, let's define the loss function for training the `projector`.\n",
    "\n",
    "**TODO**: What was the loss function for estimating projection embeddings? Please replace the `FIXME` below. Review notebook [03a_Projection.ipynb](03a_Projection.ipynb) section 3.2 for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe942748-a017-4725-8350-86cbbbe6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    rbg_img, lidar_depth, class_idx = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img)\n",
    "    lidar_emb = lidar_cnn.get_embs(lidar_depth)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ac873-88ce-4d93-8000-89f77eac929e",
   "metadata": {},
   "source": [
    "The `projector` will take a little while to train, but at the end of it, should reach a validation loss around 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19e75d32-bceb-4e8a-a785-14a9aa4348fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32, 3200])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 5.5659\n",
      "Epoch   0 | Valid Loss: 5.6017\n",
      "Epoch   1 | Train Loss: 5.5664\n",
      "Epoch   1 | Valid Loss: 5.6006\n",
      "Epoch   2 | Train Loss: 5.5661\n",
      "Epoch   2 | Valid Loss: 5.6015\n",
      "Epoch   3 | Train Loss: 5.5603\n",
      "Epoch   3 | Valid Loss: 5.5967\n",
      "Epoch   4 | Train Loss: 5.5621\n",
      "Epoch   4 | Valid Loss: 5.5976\n",
      "Epoch   5 | Train Loss: 5.5573\n",
      "Epoch   5 | Valid Loss: 5.5908\n",
      "Epoch   6 | Train Loss: 5.5545\n",
      "Epoch   6 | Valid Loss: 5.5910\n",
      "Epoch   7 | Train Loss: 5.5544\n",
      "Epoch   7 | Valid Loss: 5.5878\n",
      "Epoch   8 | Train Loss: 5.5478\n",
      "Epoch   8 | Valid Loss: 5.5890\n",
      "Epoch   9 | Train Loss: 5.5483\n",
      "Epoch   9 | Valid Loss: 5.5865\n",
      "Epoch  10 | Train Loss: 5.5464\n",
      "Epoch  10 | Valid Loss: 5.5891\n",
      "Epoch  11 | Train Loss: 5.5495\n",
      "Epoch  11 | Valid Loss: 5.5865\n",
      "Epoch  12 | Train Loss: 5.5488\n",
      "Epoch  12 | Valid Loss: 5.5871\n",
      "Epoch  13 | Train Loss: 5.5482\n",
      "Epoch  13 | Valid Loss: 5.5852\n",
      "Epoch  14 | Train Loss: 5.5495\n",
      "Epoch  14 | Valid Loss: 5.5908\n",
      "Epoch  15 | Train Loss: 5.5479\n",
      "Epoch  15 | Valid Loss: 5.5877\n",
      "Epoch  16 | Train Loss: 5.5445\n",
      "Epoch  16 | Valid Loss: 5.5865\n",
      "Epoch  17 | Train Loss: 5.5462\n",
      "Epoch  17 | Valid Loss: 5.5851\n",
      "Epoch  18 | Train Loss: 5.5465\n",
      "Epoch  18 | Valid Loss: 5.5849\n",
      "Epoch  19 | Train Loss: 5.5465\n",
      "Epoch  19 | Valid Loss: 5.5827\n",
      "Epoch  20 | Train Loss: 5.5461\n",
      "Epoch  20 | Valid Loss: 5.5834\n",
      "Epoch  21 | Train Loss: 5.5461\n",
      "Epoch  21 | Valid Loss: 5.5844\n",
      "Epoch  22 | Train Loss: 5.5488\n",
      "Epoch  22 | Valid Loss: 5.5845\n",
      "Epoch  23 | Train Loss: 5.5451\n",
      "Epoch  23 | Valid Loss: 5.5877\n",
      "Epoch  24 | Train Loss: 5.5468\n",
      "Epoch  24 | Valid Loss: 5.5833\n",
      "Epoch  25 | Train Loss: 5.5445\n",
      "Epoch  25 | Valid Loss: 5.5863\n",
      "Epoch  26 | Train Loss: 5.5454\n",
      "Epoch  26 | Valid Loss: 5.5834\n",
      "Epoch  27 | Train Loss: 5.5447\n",
      "Epoch  27 | Valid Loss: 5.5851\n",
      "Epoch  28 | Train Loss: 5.5419\n",
      "Epoch  28 | Valid Loss: 5.5831\n",
      "Epoch  29 | Train Loss: 5.5471\n",
      "Epoch  29 | Valid Loss: 5.5828\n",
      "Epoch  30 | Train Loss: 5.5446\n",
      "Epoch  30 | Valid Loss: 5.5841\n",
      "Epoch  31 | Train Loss: 5.5432\n",
      "Epoch  31 | Valid Loss: 5.5824\n",
      "Epoch  32 | Train Loss: 5.5455\n",
      "Epoch  32 | Valid Loss: 5.5819\n",
      "Epoch  33 | Train Loss: 5.5444\n",
      "Epoch  33 | Valid Loss: 5.5837\n",
      "Epoch  34 | Train Loss: 5.5453\n",
      "Epoch  34 | Valid Loss: 5.5844\n",
      "Epoch  35 | Train Loss: 5.5426\n",
      "Epoch  35 | Valid Loss: 5.5845\n",
      "Epoch  36 | Train Loss: 5.5429\n",
      "Epoch  36 | Valid Loss: 5.5817\n",
      "Epoch  37 | Train Loss: 5.5433\n",
      "Epoch  37 | Valid Loss: 5.5821\n",
      "Epoch  38 | Train Loss: 5.5442\n",
      "Epoch  38 | Valid Loss: 5.5846\n",
      "Epoch  39 | Train Loss: 5.5447\n",
      "Epoch  39 | Valid Loss: 5.5817\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "optimizer = torch.optim.Adam(projector.parameters())\n",
    "assesment_utils.train_model(projector, optimizer, get_projector_loss, epochs, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b916d-8bcf-43f0-8c06-2d12302fef05",
   "metadata": {},
   "source": [
    "Time to bring it together. Let's create a new model `RGB2LiDARClassifier` where we can use our projector with the pre-trained `lidar_cnn` model.\n",
    "\n",
    "**TODO**: Please fix the `FIXME`s below. Which `embedder` should we be using from our `CILP_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce38f00d-70a5-4d07-af19-58afebb49063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2LiDARClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.FIXME = CILP_model.FIXME\n",
    "        self.shape_classifier = lidar_cnn\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        img_encodings = self.img_embedder(imgs)\n",
    "        proj_lidar_embs = self.projector(img_encodings)\n",
    "        return self.shape_classifier(data_embs=proj_lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9978420f-2887-4b38-82c0-b3888541d3aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ContrastivePretraining' object has no attribute 'FIXME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mRGB2LiDARClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m, in \u001b[0;36mRGB2LiDARClassifier.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojector \u001b[38;5;241m=\u001b[39m projector\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFIXME \u001b[38;5;241m=\u001b[39m \u001b[43mCILP_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIXME\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_classifier \u001b[38;5;241m=\u001b[39m lidar_cnn\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ContrastivePretraining' object has no attribute 'FIXME'"
     ]
    }
   ],
   "source": [
    "my_classifier = RGB2LiDARClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3fd01-8364-4165-b344-a48c286dbe92",
   "metadata": {},
   "source": [
    "Before we train this model, let's see how it does out of the box. We'll create a function `get_correct` that we can use to calculate the number of classifications that were correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f470060-3e8e-4b43-b56e-b74fd0160c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct(output, y):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69ad75-fe12-43de-9758-94047cfa3a3d",
   "metadata": {},
   "source": [
    "Next, we can make a `get_valid_metrics` function to calculate the model's accuracy with the validation dataset. If done correctly, the accuracy should be above `.70`, or 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b467c-7a11-492b-aa69-bb3695b27e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_metrics():\n",
    "    my_classifier.eval()\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(valid_dataloader):\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "    print(f\"Valid Loss: {loss.item():2.4f} | Accuracy {correct/valid_N:2.4f}\")\n",
    "\n",
    "get_valid_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85538953-0ee3-4168-898e-baaa8f8c30e6",
   "metadata": {},
   "source": [
    "Finally, let's fine-tune the completed model. Since `CILP` and `lidar_cnn` are frozen, this should only change the `projector` part of the model. Even so, the model should achieve a validation accuracy of above `.95` or 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd4664-7520-47a2-92d0-b455d9678f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(my_classifier.parameters())\n",
    "\n",
    "my_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    correct = 0\n",
    "    batch_correct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        rbg_img, _, class_idx = batch\n",
    "        output = my_classifier(rbg_img)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n",
    "        batch_correct = get_correct(output, class_idx)\n",
    "        correct += batch_correct\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train Loss: {loss.item():2.4f} | Accuracy {correct/train_N:2.4f}\")\n",
    "    get_valid_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f7807",
   "metadata": {},
   "source": [
    "## 5.4 Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2c329",
   "metadata": {},
   "source": [
    "Moment of truth! To assess your model run the following two cells. There are ten points that are graded:abs\n",
    "\n",
    "* Confirm CILP has a validation loss of below `3.2` (5 points)\n",
    "* Confirm the `projector` can be used with `lidar_cnn` to accurately classify images. Five batches of images will be tested if the batch accuracy is above `.95`. (1 point each for 5 points total)\n",
    "\n",
    "9 out of 10 points are required to pass the assessment. Good luck!\n",
    "\n",
    "Please pass your `CILP_model` and `projector` below. If the names of these models have changed, please update the below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25834296",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(CILP_model, projector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90006a68",
   "metadata": {},
   "source": [
    "## 6.7 Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4557a",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c0679",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
