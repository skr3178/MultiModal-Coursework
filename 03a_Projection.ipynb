{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510f3537-1ddd-4ab6-96a2-a4e6f93d1453",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7dff88-ebc0-4fdf-ac4a-db72e6eb9566",
   "metadata": {},
   "source": [
    "# 3b. Cross-Modal Projection\n",
    "\n",
    "Earlier in this course, we covered a variety of different matrix algebra operations to develop multimodal model architecture. In this lab, we will be exploring a relatively advanced but powerful technique: cross-modal projection. While this technique is commonly used to project image embeddings into a text embedding space, it is not limited to this data type. This time, we will try an opposite approach. Can we project text embeddings into the image embedding space?\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Investigate the cross-model projector architecture\n",
    "* Train a cross-model projector\n",
    "* Integrate a cross-model projector into an existing model\n",
    "\n",
    "Let's load some libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce64b14-e9e7-45ea-9de1-dcb596720c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import clip\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce21031-3848-4108-b508-042c89b8a763",
   "metadata": {},
   "source": [
    "## 3.1 Setup\n",
    "### 3.1.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b87a15-1d5d-46e7-95bb-4f49d1128048",
   "metadata": {},
   "source": [
    "To project one mode onto another, it would be useful to have a model as base. Let's build a convolutional neural network to classify images while we explore our dataset. We can use [VGG16](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html), a pre-trained image classifier, as a base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd541dcf-4d5c-4407-abe9-b2125a66c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the VGG16 network *pre-trained* on the ImageNet dataset\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)\n",
    "vgg_model.requires_grad_(False) # Freeze model during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd7626-3fb7-4b8d-8b08-6bd53298e84e",
   "metadata": {},
   "source": [
    "VGG16 comes with a set of [torchvision transforms](https://pytorch.org/vision/0.9/transforms.html) we can use to convert images into the shape and format VGG16 can ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e5aaf-93ae-4782-9c2b-a1eaffef9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()\n",
    "pre_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e17683-63e5-4fe5-a50f-82eea4f58ea1",
   "metadata": {},
   "source": [
    "Before applying these transformations to all of our data, let's test it on one example. We have a few different flower photos located in the `data/flower_photos/` folder. These photos were taken at the [Elizabeth F. Gamble Garden](https://www.gamblegarden.org/visit-us/overview/) in Palo Alto. Please visit if you get the chance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88526f2f-ab1b-45db-8477-da7acdff183e",
   "metadata": {},
   "source": [
    "<center><img src=\"data/flower_photos/PXL_20250104_202132415.jpg\" width=\"25%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5042b-de7c-4ad1-a907-578786de781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/flower_photos/PXL_20250104_202132415.jpg\"\n",
    "image = Image.open(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ace612-a857-4796-affa-fe177c876e04",
   "metadata": {},
   "source": [
    "The following code will apply the transforms and print out the result. Because the image is being normalized, the result will look a bit abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d0bb0-6695-4de8-865a-d4026e570b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original image shape: ', mpimg.imread(img_path).shape)\n",
    "processed_image = tv_io.read_image(img_path).to(device)\n",
    "processed_image = pre_trans(image)  # weights.transforms()\n",
    "print(\"Processed image shape: \", processed_image.shape)\n",
    "pil_image = torchvision.transforms.functional.to_pil_image(processed_image)\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc86261-34d0-4eaa-aa15-3246ffb935b4",
   "metadata": {},
   "source": [
    "Speaking of image transforms, let's define a little bit of data augmentation. We'll randomly [crop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) and [flip](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomHorizontalFlip.html) the image to add more variety to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08713b16-4433-4969-aab9-55d3c9fa68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.8, 1), ratio=(1, 1)),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c976c45-c606-43c4-ae99-49ee7afc4b0b",
   "metadata": {},
   "source": [
    "We can scale these steps for all of our flower photos. Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dba498-430a-416c-a737-82a99f5ae98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "df = pd.read_csv(DATA_DIR + 'flower_photos.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b69c7-642d-44bd-895b-4a8fb05bf643",
   "metadata": {},
   "source": [
    "There are `3` different kinds of flowers in our dataset:\n",
    "* [Kniphofia Uvaria](https://en.wikipedia.org/wiki/Kniphofia_uvaria)\n",
    "* [Salvia Splendens](https://en.wikipedia.org/wiki/Salvia_splendens)\n",
    "* [Tagetes Patula](https://en.wikipedia.org/wiki/Tagetes_patula)\n",
    "\n",
    "This dataset is not very large, and that's ok. Because we're using a pre-trained model, the model will have enough prior context in order to learn from these images. We will also purposely overfit this data to verify to architecture can learn on this kind of data. Once we see that the model is learning, we can expand our dataset and create validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20899176-942c-4de0-a78f-159c0613964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_length = 150\n",
    "patches = 3\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.imgs = []\n",
    "        self.descriptions = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for idx in range(len(df)):\n",
    "            row = df.loc[idx]\n",
    "            img = Image.open(DATA_DIR + \"flower_photos/\" + row[\"Filename\"])\n",
    "            self.imgs.append(pre_trans(img).to(device))\n",
    "            text_patches = [row[\"Description\"][t*patch_length:(t+1)*patch_length] for t in range(patches)]\n",
    "            self.descriptions.append(text_patches)\n",
    "            label = torch.tensor(row[\"Class ID\"]).to(device)\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        description = self.descriptions[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, description, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50dae71-abff-441c-bf4a-dbc7f450c842",
   "metadata": {},
   "source": [
    "Now that we've defined our dataset, let's initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2a2c0-8e00-453d-90c0-6780d298ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_data = MyDataset()\n",
    "dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "train_N = len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46734cf7-80bf-4f26-8cce-6d3abc6bea0c",
   "metadata": {},
   "source": [
    "### 3.1.2 The Image Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37c474-e8ba-4fc0-99c2-c33e4de6a448",
   "metadata": {},
   "source": [
    "Time to make our image classification model. We'll use most of the VGG16 model, but we'll remove the last few layers and add our own instead. The vector output of the VGG16 portion of the model will become our `embedder` before being fed to the rest of the model.`\n",
    "\n",
    "To make it easier for other developers to use this embedding, we will define a `get_img_embs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab5990-4b8a-4d2e-b023-7492a2999af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 3\n",
    "\n",
    "class FlowerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedder = nn.Sequential(\n",
    "            vgg_model.features,\n",
    "            vgg_model.avgpool,\n",
    "            nn.Flatten(),\n",
    "            vgg_model.classifier[0:3]\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, N_CLASSES)\n",
    "        )\n",
    "\n",
    "    def get_img_embs(self, imgs):\n",
    "        return self.embedder(imgs)\n",
    "    \n",
    "    def forward(self, imgs=None, img_embs=None):\n",
    "        assert (imgs is not None or img_embs is not None), \"No images or embeddings given.\"\n",
    "        if imgs is not None:\n",
    "            img_embs = self.get_img_embs(imgs)\n",
    "        return self.classifier(img_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc28c1e-93b7-4af7-988a-df5dea2150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_classifier = FlowerClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1be31-932a-4e95-9a32-42fa1005cd02",
   "metadata": {},
   "source": [
    "Almost done. Let's define out loss function. Since we're classifying these photos into multiple categories, we should use the [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437d72d-26cc-46bd-9891-169f8d5db2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier_loss(model, batch):\n",
    "    imgs, _, labels = batch\n",
    "    pred_labels = model(imgs=random_trans(imgs))\n",
    "    accuracy = get_batch_accuracy(pred_labels, labels, train_N)\n",
    "    return nn.CrossEntropyLoss()(pred_labels, labels), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18aad5d-4b04-4ad8-a27a-97048dd6646b",
   "metadata": {},
   "source": [
    "In `get_classifier_loss`, there is a yet-to-be-defined `get_batch_accuracy` function. Let's define that now. It'll be easier to tell how well the model is performing with the accuracy function instead of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab323ef2-aa91-4ef9-b39a-6106a7088d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(pred, label, N):\n",
    "    y = pred.argmax(dim=-1)\n",
    "    correct = y.eq(label).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d255c42-17c5-4f42-8866-5b5e28b1615a",
   "metadata": {},
   "source": [
    "Finally, let's define the function to train the model. We'll be reusing this function to train multiple models, so let's make it flexible by adding the loss function (`loss_fn`) as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aaf5d-aca8-44fd-bf11-33dbbcdef51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, epochs=20, print_accuracy=False):\n",
    "    lrate = 0.01\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        accuracy = 0\n",
    "        batch_accuracy = 0\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss, batch_accuracy = loss_fn(model, batch)\n",
    "            accuracy += batch_accuracy\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        out_string = f\"Epoch {epoch:3d} | Loss: {loss.item():2.4f}\"\n",
    "        if print_accuracy:\n",
    "            out_string += f\" | Accuracy {accuracy:2.4f}\"\n",
    "        print(out_string)\n",
    "    \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3e390-a0ae-4d89-a297-171638b75d66",
   "metadata": {},
   "source": [
    "Moment of truth! Let's see how well this classifier learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e9eaf-56f0-471d-a930-4ce7dc094c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(flower_classifier, get_classifier_loss, epochs=10, print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715bce9-6f2d-48d1-9405-c71f1a228fa2",
   "metadata": {},
   "source": [
    "Due to the randomness behind model training, different runs can have different results. When we ran it, we got an accuracy of about `0.9697`, which is about `96.97%`. Not bad! This will be our benchmark going forward. Let's see if we can change the modality of this model while maintaining this accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdad3f-1f2b-405e-9fb4-7d4765724682",
   "metadata": {},
   "source": [
    "## 3.2 Cross-Modal Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d26ab-5958-472a-bc1e-a87e20cd4ce2",
   "metadata": {},
   "source": [
    "Now that we have a base model, let's see how we can change its modality. If a model that projects image embeddings onto text embeddings is called a Vision Language Model (VLM), let's call a model that projects text embeddings onto image embeddings a Language Vision Model (LVM).\n",
    "\n",
    "To start, let's find a way to get text embeddings. Since it is already designed to correlate text and images, let's use [CLIP](https://github.com/openai/CLIP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105a12c-4c8b-4154-95e4-27ccb31c0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\")\n",
    "clip_model.eval()\n",
    "CLIP_FEATURES = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ec907-042d-40f5-9395-bcce710095df",
   "metadata": {},
   "source": [
    "Before we create a model with CLIP, let's get some practice. First, we'll create a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649cd71-ef07-4eb3-89c9-66bf23dd49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"A Kniphofia Uvaria flower\",\n",
    "    \"A Salvia Splendens flower\",\n",
    "    \"A Tagetes Patula flower\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10a3fd-1d42-469a-b42e-9074522c1ef3",
   "metadata": {},
   "source": [
    "Then, we'll run it through CLIP to get an encoding. In this case, we'll use our encoding as our embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5c281-982e-4044-ae53-fa1666deed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = clip.tokenize(text_list).to(device)\n",
    "clip_text_encodings = clip_model.encode_text(text_tokens).float()\n",
    "clip_text_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0f313-c835-44b3-a68d-325075c158dc",
   "metadata": {},
   "source": [
    "To project CLIP embeddings onto VGG16 embeddings, we should note both the CLIP embedding size and the VGG16 embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771af0f9-9ba7-45d7-b4f3-ea24c5e6a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_emb_size = clip_text_encodings[0].shape\n",
    "clip_emb_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ee927-806b-4382-bc5d-cc323615e788",
   "metadata": {},
   "source": [
    "CLIP has a maximum token length of 77, but our text descriptions of our flowers are much larger than that. In order to add more information, we'll break our text descriptions down into `patches`. Not only does it allow us to analyze a larger description, but we can also spatially break down the reasoning of our text.\n",
    "\n",
    "Because of this patching, we will run CLIP multiple times per row of description in our dataset. Let's make a function (`get_clip_encodings`) to run through the list of patches. We can use python [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) to make the operation independent of the patch size. Then, we can take our list of encodings and concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8d424-1b5f-41cd-9b86-fd87d58a5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_encodings(texts):\n",
    "    text_tokens = [clip.tokenize(t).to(device) for t in texts]\n",
    "    clip_text_encodings = [clip_model.encode_text(t).float() for t in text_tokens]\n",
    "    return torch.cat(clip_text_encodings, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b12ecc-681f-4bbe-b71f-ff499988ff6d",
   "metadata": {},
   "source": [
    "There are multiple ways we could deduce the VGG16 embedding size. We could look at the model summary. Another way is to feed an image through our `get_img_embs` and check the size of the output. We can batch our `processed_image` with [unsqueeze](https://pytorch.org/docs/main/generated/torch.unsqueeze.html) to test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f5a6b-62db-4e95-afbf-9d8a8e435cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torch.unsqueeze(processed_image, 0).to(device)\n",
    "imb_embs = flower_classifier.get_img_embs(test_image)\n",
    "vgg_shape = imb_embs[0].shape\n",
    "vgg_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726a351-a7a9-48b5-86bf-37c868fd0f87",
   "metadata": {},
   "source": [
    "Now that we know the size we're converting from and the size we're converting to, we can make our modal `projector`. We can use any neural network operations we want as long as our input dimensions and our output dimensions reflect this conversion. It may be surprising to see we only need a few linear layers. Even state-of-the-art Vision Language Models like LLaVA only use a few linear layers in [their projector](https://github.com/huggingface/transformers/blob/v4.49.0/src/transformers/models/llava/modeling_llava.py#L88)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094a8cd-faf2-4d83-8e29-82d6321f4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = nn.Sequential(\n",
    "    nn.Linear(clip_emb_size[0]*patches, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, vgg_shape[0])\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd36b3-4bc3-4e5e-a1f0-600a1e849c38",
   "metadata": {},
   "source": [
    "We will use our image-description pairs to train the projector. The text description will be fed through CLIP and the projector while the corresponding image will be fed through the `flower_classifier`'s `get_img_embs` function.\n",
    "\n",
    "Since we're trying to minimize the distance between our predicted image embeddings and the true image embeddings, our loss function will change. Even though we're working in an abstract image and text space, the math is not much different than our robotics position predictor from earlier in the course. Like in those previous labs, we will use the [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa380ad4-e259-4a5c-9bd8-7e7a5f92680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_loss(model, batch):\n",
    "    imgs, texts, _ = batch\n",
    "    imb_embs = flower_classifier.get_img_embs(imgs)\n",
    "\n",
    "    text_encodings = get_clip_encodings(texts)\n",
    "    pred_img_embs = model(text_encodings).to(device)\n",
    "    return nn.MSELoss()(pred_img_embs, imb_embs), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c010f1f-5edc-42e9-a00c-97531e829600",
   "metadata": {},
   "source": [
    "Time to train the model! Let's see how well we can project these text embeddings into image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4e46f2-c7d1-4394-bec7-9e2b095db656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(projector, get_projector_loss, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dce579-b3e8-41e5-8bc9-9a85816c527a",
   "metadata": {},
   "source": [
    "## 3.3 A Language Vision Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82af9605-8556-491a-a71a-3bd2de1b2fe1",
   "metadata": {},
   "source": [
    "It looks like a projector model was able to learn something, but it is difficult to say how accurate its predictions are. Let's connect it to our `flower_classifier` in order to test how much it learned.`\n",
    "\n",
    "We can create a new model `MyLVM` to feed the output of our projector as image embeddings in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5ea33-66a1-44df-9921-64a90c4626bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLVM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projector = projector\n",
    "        self.flower_classifier = flower_classifier\n",
    "    \n",
    "    def forward(self, texts):\n",
    "        text_encodings = get_clip_encodings(texts)\n",
    "        proj_img_embs = self.projector(text_encodings).to(device)\n",
    "        return self.flower_classifier(img_embs=proj_img_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c482724-37fd-44c0-87b2-3cb7d87798e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lvm = MyLVM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38946cd-128f-4da4-ab26-69e20df32fce",
   "metadata": {},
   "source": [
    "Now that we're back to using a classifier model, let's update our loss function. This time, we'll take our text descriptions and feed it into our model to see how well it can identify the correct flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbc9e7-5915-4f2b-b898-3d67f1e8e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LVM_loss(model, batch):\n",
    "    _, texts, labels = batch\n",
    "    pred_labels = model(texts)\n",
    "    accuracy = get_batch_accuracy(pred_labels, labels, train_N)\n",
    "    return nn.CrossEntropyLoss()(pred_labels, labels), accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2913d-8fc1-4362-be1e-1eb4b2458dee",
   "metadata": {},
   "source": [
    "Before we do anymore training, let's see how the model does as is. We'll one through one epoch of the dataset to calculate our total accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9837b4c-c80d-4120-81d0-8fa33973cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lvm.eval()\n",
    "accuracy = 0\n",
    "batch_accuracy = 0\n",
    "for step, batch in enumerate(dataloader):\n",
    "    loss, batch_accuracy = get_LVM_loss(my_lvm, batch)\n",
    "    accuracy += batch_accuracy\n",
    "\n",
    "out_string = f\"Loss: {loss.item():2.4f} | Accuracy {accuracy:2.4f}\"\n",
    "print(out_string) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e6ff2cc-c6d6-4bdb-b1b2-b14681eb7ef9",
   "metadata": {},
   "source": [
    "Not bad again! Even though the loss of the projector isn't perfect, it does a good enough job for the `flower_classifier` to understand what is being passed to it.\n",
    "\n",
    "In the paper [Vision Instruction Tuning](https://arxiv.org/abs/2304.08485), it describes a two step approach to training a cross-modal projection model. The first-step we already did: training a projector on embedding pairs. The second step is to fine-tune the entire VLM. Let's replicate the second step for our LVM. This trains the whole pipeline from the inputed text to the output flower prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd99ad-9607-4f15-b6c4-f1f090e27b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(my_lvm, get_LVM_loss, epochs=30, print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14956e-7a14-4be0-85a5-7f1bba47e435",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c11bb5-6e0f-4307-b7df-1cc35c1bf8ad",
   "metadata": {},
   "source": [
    "Congrats on finishing the lab! This concludes the more mathematical theory section of the course. In the next few labs, we will be reviewing how multi-modal models are used in practice. We're going to need a lot of resources to run these models. Please restart the kernel before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a175ebc-480c-4b67-a964-b78e55d4c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c0e90-a685-47a7-b275-09c968763d29",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
