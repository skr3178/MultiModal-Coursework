{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-a8GRH8DXKk"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTQNPPpeDaOk"
   },
   "source": [
    "# 2a. Intermediate Fusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81PfOMFwKBZG"
   },
   "source": [
    "In the last lab, we successfully created a multimodal model. However, the performance was not much better than a single modal model. In this notebook, we will explore a more challenging dataset and different fusion techniques to overcome this challenge.\n",
    "\n",
    "These experiments will take a moment to run. While these models are training, please continue to read on and try to predict what the results will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pkc4vj3csv3"
   },
   "source": [
    "#### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Compare four types of multimodal fusion techniques:\n",
    "  * Early fusion\n",
    "  * Late fusion\n",
    "  * Intermediate fusion with concatenation\n",
    "  * Intermediate fusion with matrix multiplication\n",
    "\n",
    "Let's begin by loading the libraries necessary for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 6260,
     "status": "error",
     "timestamp": 1689940390619,
     "user": {
      "displayName": "Devesh Khandelwal US",
      "userId": "16954520040589783180"
     },
     "user_tz": 420
    },
    "id": "NNmWEhrB-uSm",
    "outputId": "6450de47-6418-4268-82b0-da2f8e3b9fe0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPy_img\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzHZsOWLCdmB"
   },
   "source": [
    "## 2.1 The Dataset\n",
    "\n",
    "In the previous lab, our dataset was composed of three different shapes: a cube, a sphere, and a torus. Because the LiDAR data can more directly calculate the center of each object, the RGB data was ignored in our multimodal models.\n",
    "\n",
    "In this lab, we will use three of the same shape: a red cube, a green cube, and a blue cube. Since LiDAR systems cannot see color, a model based only on LiDAR data would have a hard time distinguishing which cube is which. On the other hand, RGB data would be able to distinguish between the cubes, but an RGB model would have a harder time calculating position in general. Let's see if we can create a model with the best of both worlds.\n",
    "\n",
    "Before we create any multimodal models, let's see how single modal models would perform on this dataset. In the interest of time, we've already ran a training simulation and collected the results. If you would like to verify, this can be done by running the previous notebook, but changing the data directory to `data/replicator_data_cubes/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mode_data = np.genfromtxt(\"data/cubes_only_single_mode_results.csv\", delimiter=',', skip_header=1)\n",
    "\n",
    "plot_x = range(len(single_mode_data))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(plot_x, single_mode_data[:, 1], \"green\", label = \"RGB Train Loss\")\n",
    "plt.plot(plot_x, single_mode_data[:, 2], \"darkgreen\", label = \"RGB Valid Loss\")\n",
    "plt.plot(plot_x, single_mode_data[:, 3], \"orchid\", label = \"XYZ Train Loss\")\n",
    "plt.plot(plot_x, single_mode_data[:, 4], \"darkorchid\", label = \"XYZ Valid Loss\")\n",
    "plt.title(\"Cubes Only Single Mode Results\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset, a model based on RGB images has a validation loss of about 6. We have 3 objects, so the predicted position of each object is off by 2 units. Omniverse position units are relative, meaning it's not tied to a real-world unit of measurement like feet or centimeters. Many 3D modelers and applications suggest 1 unit should be 1 meter.\n",
    "\n",
    "On the other hand, the LiDAR model achieved a loss of less than 1 on the train dataset, but over 8 on the validation dataset. This is a classic case of overfitting. Because the LiDAR model can't find the logical pattern to calculate the position of these objects, it instead attempted to memorize the train dataset.\n",
    "\n",
    "So, to put things into perspective, our objects have a 10 unit by 10 unit range it can randomly place itself in along each of the X, Y, and Z axis. In other words, the objects spawn in a 1000 unit cubed area, and the diagonal along that area is about 17.3 units. In this case, the loss is the same as our error. If the average error for each object is 2 units, that's about 11.6% of the length of diagonal range. Not bad, but not great either.\n",
    "\n",
    "Let's try to visualize this better. Below is the first set of positions for the red, green and blue cubes. The lighter points represent to correct positions. The darker points represent those same points offset by 2 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "x = [3.431835889816284, -1.59285056591033, 2.45306801795959, 3.13458254, -2.49218181591033, 2.997167998]\n",
    "y = [2.27847838401794, 3.99227786064147, -1.98521077632904, 1.551694174, 4.414836401, -1.190015916]\n",
    "z = [2.73768424987792, -4.99803066253662, -3.022789478302, 2.12207423, -4.875654503, -2.755063558]\n",
    "c = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [.5, 0, 0], [0, .5, 0], [0, 0, .5]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-6, 6)\n",
    "ax.set_zlim(-6, 6)\n",
    "\n",
    "file_path = 'images/errors.gif'\n",
    "utils.animate_3D_points(file_path, fig, ax, x, y, z, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPy_img(open(file_path,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has the same structure as last time: the first set of values are the `rgb_img`s, the second set is the `lidar_xyza` data, and the third set of values are the object positions. Let's verify that the shapes are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIRaYbZkDKQa"
   },
   "outputs": [],
   "source": [
    "train_data, train_dataloader, valid_data, valid_dataloader = utils.get_replicator_dataloaders(\"data/replicator_data_cubes/\")\n",
    "\n",
    "for i, sample in enumerate(train_data):\n",
    "    print(i, *(x.shape for x in sample))\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Composition\n",
    "\n",
    "Time to get these experiments rolling. We're going to test the performance of four models:\n",
    "* `early_net`: A multimodal model using early fusion\n",
    "* `late_net`: A multimodal model using late fusion\n",
    "* `cat_net`: A multimodal model using intermediate fusion with [concatenation](https://pytorch.org/docs/main/generated/torch.cat.html)\n",
    "* `matmul_net`: A multimodal model using intermediaet fusion with [matrix multiplication](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "\n",
    "### 2.2.1 EarlyNet\n",
    "\n",
    "First, let's make a generic convolutional neural network that we can use with both `early_net` and `late_net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positions = 9\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        kernel_size = 3\n",
    "        super().__init__()\n",
    "        flattened_size = 200 * 8 * 8\n",
    "        self.conv1 = nn.Conv2d(in_ch, 50, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        self.conv3 = nn.Conv2d(100, 200, kernel_size, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(flattened_size, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, num_positions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a function in [utils.py](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) called `train_model` to train each of our experiments. It still needs a function to extract the appropriate information from the dataloader. In the case of `early_net`, the rgb and xyz values should be [concatenated](https://pytorch.org/docs/main/generated/torch.cat.html) before feeding the data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XIU9PzSJEdTp"
   },
   "outputs": [],
   "source": [
    "def get_early_inputs(batch):\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    inputs_mm_early = torch.cat((inputs_rgb, inputs_xyz), 1)\n",
    "    return (inputs_mm_early,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the first experiment. Please run the cell below and continue reading. Training will take some time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "early_net = Net(8).to(device)\n",
    "early_opt = Adam(early_net.parameters(), lr=0.0001)\n",
    "early_train_losses, early_valid_losses = utils.train_model(\n",
    "    early_net,\n",
    "    early_opt,\n",
    "    get_early_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 LateNet\n",
    "\n",
    "Next, let's create `late_net`. This will use two of our generic `Net`s that we created in the last section. Unlike the last lab, these nets will not be pretrained, allowing for a fairer comparison with `early_net`. However, since there are two `Net`s, `late_net` has nearly twice as many paramaters as `early_net`, so a perfectly fair comparison would be difficult to achieve. It's still worthwhile to try new things. Let's see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_net = Net(4).to(device)\n",
    "xyz_net = Net(4).to(device)\n",
    "\n",
    "class LateNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rgb = rgb_net\n",
    "        self.xyz = xyz_net\n",
    "        self.fc1 = nn.Linear(num_positions * 2, num_positions * 10)\n",
    "        self.fc2 = nn.Linear(num_positions * 10, num_positions)\n",
    "\n",
    "    def forward(self, x_rgb, x_xyz):\n",
    "        x_rgb = self.rgb(x_rgb)\n",
    "        x_xyz = self.xyz(x_xyz)\n",
    "        x = torch.cat((x_rgb, x_xyz), 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of these experiments, we will use the same `get_inputs` function. `late_net`, like `cat_net` and `matmul_net`, has two different paths for the data to flow through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(batch):\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    return (inputs_rgb, inputs_xyz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define `late_net` and run the experiment. Please run the cell below and read on. Because `late_net` has more parameters, this also means it will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_net = LateNet().to(device)\n",
    "late_opt = Adam(late_net.parameters(), lr=0.0001)\n",
    "late_train_losses, late_valid_losses = utils.train_model(\n",
    "    late_net,\n",
    "    late_opt,\n",
    "    get_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 CatNet\n",
    "\n",
    "For the next experiment, we'll try something completely new. Instead of joining our data streams at the beginning or end of the model, we'll mix them together somewhere in the middle. In this case, we'll have two convolution paths: one for RGB data, and one for XYZ data. We can almost think of a convolution function as taking in some sheets of paper and outputting a different number of sheets of paper. It's like we're taking these output sheets from our two data streams and stacking them on top of each other.\n",
    "\n",
    "Before moving on, take a moment to look at the architecture below. Can you see where the two pathways meet? It doesn't appear in the `__init__` function. That `cat` operation occurs in the `forward` pass. In the generic `Net` architecture, the last convolution had `200` kernels. In this case we have `2` final convolutions with `100` kernels each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatIntermediateNet(nn.Module):\n",
    "    def __init__(self, rgb_ch, xyz_ch):\n",
    "        kernel_size = 3\n",
    "        num_positions = 9\n",
    "        super().__init__()\n",
    "        self.rgb_conv1 = nn.Conv2d(rgb_ch, 25, kernel_size, padding=1)\n",
    "        self.rgb_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n",
    "        self.rgb_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        \n",
    "        self.xyz_conv1 = nn.Conv2d(xyz_ch, 25, kernel_size, padding=1)\n",
    "        self.xyz_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n",
    "        self.xyz_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(200 * 8 * 8, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, num_positions)\n",
    "\n",
    "    def forward(self, x_rgb, x_xyz):\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv1(x_rgb)))\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv3(x_rgb)))\n",
    "        \n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv1(x_xyz)))\n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv2(x_xyz)))\n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv3(x_xyz)))\n",
    "        \n",
    "        x = torch.cat((x_rgb, x_xyz), 1)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of this architecture looks like late fusion, but the bottom of it looks like early fusion. That means the number of parameters for `cat_net` is less that `late_net`. However, more parameters are not always better. Let's see if this is true by running the cell below. This experiment will take some time. Please continue to read on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "cat_net = ConcatIntermediateNet(4, 4).to(device)\n",
    "cat_net_opt = Adam(cat_net.parameters(), lr=0.0001)\n",
    "cat_net_train_losses, cat_net_valid_losses = utils.train_model(\n",
    "    cat_net,\n",
    "    cat_net_opt,\n",
    "    get_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 MatmulNet\n",
    "\n",
    "We're finally at our last experiment, `matmul_net`. This architecture is very similar to `cat_net`, but with one key difference. Instead of concatenating the outputs of the two final convolutions, we're going to broadcast [matrix multiply](https://en.wikipedia.org/wiki/Matrix_multiplication) the outputs together. Since they're both squares of the same shape, this is a valid operation.\n",
    "\n",
    "The [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)), where we multiply the matrices elementwise, would also be valid. However, matrix multiplication allows for different parts of the matrices to interact with each other, increasing the chance our network finds a useful multiplication operation.\n",
    "\n",
    "<center><img src=\"images/matmul.png\" width=\"50%\" /></center>\n",
    "\n",
    "Since we're matrix multiplying instead of concatenating, our `2` x `100` convolution output gets reduced down to `100` matrices. As they are still matrices, they should be `flattened` before passing the information to the dense `linear` section of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatmulIntermediateNet(nn.Module):\n",
    "    def __init__(self, rgb_ch, xyz_ch):\n",
    "        kernel_size = 3\n",
    "        color_chs = 9\n",
    "        num_positions = 9\n",
    "        super().__init__()\n",
    "        self.rgb_conv1 = nn.Conv2d(rgb_ch, 25, kernel_size, padding=1)\n",
    "        self.rgb_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n",
    "        self.rgb_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        \n",
    "        self.xyz_conv1 = nn.Conv2d(xyz_ch, 25, kernel_size, padding=1)\n",
    "        self.xyz_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n",
    "        self.xyz_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(100 * 8 * 8, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.fc3 = nn.Linear(100, num_positions)\n",
    "\n",
    "    def forward(self, x_rgb, x_xyz):\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv1(x_rgb)))\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))\n",
    "        x_rgb = self.pool(F.relu(self.rgb_conv3(x_rgb)))\n",
    "        \n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv1(x_xyz)))\n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv2(x_xyz)))\n",
    "        x_xyz = self.pool(F.relu(self.xyz_conv3(x_xyz)))\n",
    "        \n",
    "        x = torch.matmul(x_rgb, x_xyz)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "matmul_net = MatmulIntermediateNet(4, 4).to(device)\n",
    "matmul_net_opt = Adam(matmul_net.parameters(), lr=0.0001)\n",
    "matmul_net_train_losses, matmul_net_valid_losses = utils.train_model(\n",
    "    matmul_net,\n",
    "    matmul_net_opt,\n",
    "    get_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    valid_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Results\n",
    "\n",
    "Congrats on making it to the end. There's a good chance at this point, the experiment is still running. For now, please move on to the [next notebook](02b_Contrastive_Pretraining.ipynb) in the meantime. Please return when you are finished to see the comparison graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(plot_x, early_valid_losses, \"goldenrod\", label = \"EarlyNet\")\n",
    "plt.plot(plot_x, late_valid_losses, \"green\", label = \"LateNet\")\n",
    "plt.plot(plot_x, cat_net_valid_losses, \"blue\", label = \"CatNet\")\n",
    "plt.plot(plot_x, matmul_net_valid_losses, \"orchid\", label = \"MatmulNet\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned a few interesting fusion techniques in this notebook, but we're not done yet. In the next section, we'll learn how to project the embeddings of one model into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
